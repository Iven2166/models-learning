{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "# np/pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# transformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# CRF\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "global_vectors = GloVe(name='840B', dim=300, cache='../../data/Glove840B300d/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 300])\n"
     ]
    }
   ],
   "source": [
    "print(global_vectors.get_vecs_by_tokens(['the','world','will','be','better']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探查conll2003数据\n",
    "\n",
    "定义dataset，dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b854c11f47f49ae8d282c8ca372c228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_udpos = torchtext.datasets.UDPOS(root='./torchtext_datasets_udpos/', split=('train','valid','test'))\n",
    "dataset_conll2003 = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['SOCCER',\n",
       "  '-',\n",
       "  'JAPAN',\n",
       "  'GET',\n",
       "  'LUCKY',\n",
       "  'WIN',\n",
       "  ',',\n",
       "  'CHINA',\n",
       "  'IN',\n",
       "  'SURPRISE',\n",
       "  'DEFEAT',\n",
       "  '.'],\n",
       " 'pos_tags': [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7],\n",
       " 'chunk_tags': [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0],\n",
       " 'ner_tags': [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_conll2003['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DatasetDict.class_encode_column of DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_conll2003.class_encode_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag2id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, \n",
    "              'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8, START_TAG:9, STOP_TAG:10}\n",
    "\n",
    "ner_id2tag = {}\n",
    "for key in ner_tag2id.keys():\n",
    "    ner_id2tag[ner_tag2id[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC',\n",
       " 9: '<START>',\n",
       " 10: '<STOP>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
       " ['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ner_id_to_tags(ner_id_seq):\n",
    "    res1, res2 = [], []\n",
    "    for ner_id in ner_id_seq:\n",
    "        res1.append(ner_id2tag.get(ner_id, ''))\n",
    "        res2.append(ner_id2tag.get(ner_id, '-').split('-')[-1])\n",
    "    return res1, res2\n",
    "\n",
    "ner_id_to_tags(dataset_conll2003['train'][0]['ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length = 124\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "for k in dataset_conll2003.keys():\n",
    "    for i in dataset_conll2003[k]['tokens']:\n",
    "        m = max(m, len(i))\n",
    "print('max length = {}'.format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30291\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for k in dataset_conll2003.keys():\n",
    "    for tokens in dataset_conll2003[k]['tokens']:\n",
    "        for token in tokens:\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "\n",
    "for token in [START_TAG, STOP_TAG]:\n",
    "    if token not in word_to_ix:\n",
    "        word_to_ix[token] = len(word_to_ix)\n",
    "        \n",
    "print(len(word_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well-fancied', '<START>', '<STOP>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_ix.keys())[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 3, 0, 7, 0, 0, 0, 7, 0, 0, 10]\n"
     ]
    }
   ],
   "source": [
    "t = dataset_conll2003['train'][0]['ner_tags']\n",
    "t.insert(0, ner_tag2id[START_TAG])\n",
    "t.append(ner_tag2id[STOP_TAG])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_word2ix(word_to_ix, token_list):\n",
    "    res = list()\n",
    "    for token in token_list:\n",
    "        res.append(word_to_ix.get(token, len(word_to_ix)+1))\n",
    "    return {'token_ids':res}\n",
    "\n",
    "def fill_start_end_tag(ner_tags_tmp):\n",
    "    ner_tags_curr = ner_tags_tmp.copy()\n",
    "    ner_tags_curr.insert(0,  ner_tag2id[START_TAG])\n",
    "    ner_tags_curr.append(ner_tag2id[STOP_TAG])\n",
    "    return {'ner_tags_fill': ner_tags_curr}\n",
    "\n",
    "\n",
    "def func_get_new_cols(word_to_ix, token_list, ner_tags_tmp):\n",
    "    # get token ids\n",
    "    res = list()\n",
    "    res.append(word_to_ix.get(START_TAG))\n",
    "    for token in token_list:\n",
    "        res.append(word_to_ix.get(token, len(word_to_ix)+1))\n",
    "    res.append(word_to_ix.get(STOP_TAG))\n",
    "    \n",
    "    # fill the tokens \n",
    "    token_list.insert(0, START_TAG)\n",
    "    token_list.append(STOP_TAG)\n",
    "    \n",
    "    # fill the ner tags\n",
    "    ner_tags_curr = ner_tags_tmp.copy()\n",
    "    ner_tags_curr.insert(0,  ner_tag2id[START_TAG])\n",
    "    ner_tags_curr.append(ner_tag2id[STOP_TAG])\n",
    "    return {'token_ids':res, 'ner_tags_fill': ner_tags_curr, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30289"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix.get(START_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-98d7a3e3f12bd0c0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-ac8336dbe7d3dd9f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-7798d4403bff3e48.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_conll2003_train=dataset_conll2003['train'].map(lambda x: func_get_new_cols(word_to_ix, x['tokens'], x['ner_tags']))\n",
    "dataset_conll2003_train.set_format(type=\"torch\", columns=['token_ids','ner_tags_fill'])\n",
    "\n",
    "dataset_conll2003_test=dataset_conll2003['test'].map(lambda x: func_get_new_cols(word_to_ix, x['tokens'], x['ner_tags']))\n",
    "dataset_conll2003_test.set_format(type=\"torch\", columns=['token_ids','ner_tags_fill'])\n",
    "\n",
    "dataset_conll2003_val=dataset_conll2003['validation'].map(lambda x: func_get_new_cols(word_to_ix, x['tokens'], x['ner_tags']))\n",
    "dataset_conll2003_val.set_format(type=\"torch\", columns=['token_ids','ner_tags_fill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  1,  2, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_conll2003_test['ner_tags_fill'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<START>',\n",
       "  'EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.',\n",
       "  '<STOP>'],\n",
       " tensor([30289,     0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
       "         30290]),\n",
       " tensor([ 9,  3,  0,  7,  0,  0,  0,  7,  0,  0, 10]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_conll2003_train['tokens'][0], dataset_conll2003_train['token_ids'][0],  dataset_conll2003_train['ner_tags_fill'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'token_ids', 'ner_tags_fill'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_conll2003_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.token_ids = self.data['token_ids'] # 在这变成torch.tensor，但长度不同\n",
    "        self.ner_tags_fill = self.data['ner_tags_fill']\n",
    "        self.tokens = self.data['tokens']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         curr = dict()\n",
    "#         curr['token_ids'] = self.token_ids\n",
    "#         curr['ner_tags'] = self.ner_tags\n",
    "        return self.token_ids[index], self.ner_tags_fill[index], self.tokens[index]\n",
    "\n",
    "\n",
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    Padds batch of variable length\n",
    "\n",
    "    note: it converts things ToTensor manually here since the ToTensor transform\n",
    "    assume it takes in images rather than arbitrary tensors.\n",
    "    '''\n",
    "    x, y, z = zip(*batch)\n",
    "    x_lens = [len(x_i) for x_i in x]\n",
    "    y_lens = [len(y_i) for y_i in y]\n",
    "    x_pad = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y_pad = torch.nn.utils.rnn.pad_sequence(y, batch_first=True)\n",
    "    return x_pad, torch.tensor(x_lens), y_pad, torch.tensor(y_lens), z\n",
    "    \n",
    "dataset_conll2003_train_loader = DataLoader(\n",
    "    MyDataset(dataset_conll2003_train),\n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    collate_fn=lambda x: collate_fn_padd(x))\n",
    "\n",
    "\n",
    "dataset_conll2003_test_loader = DataLoader(\n",
    "    MyDataset(dataset_conll2003_test),\n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    collate_fn=lambda x: collate_fn_padd(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_conll2003_val_loader = DataLoader(\n",
    "    MyDataset(dataset_conll2003_val),\n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    collate_fn=lambda x: collate_fn_padd(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30289,  4936,    70,  4935,  2058, 30290,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]) tensor(6) tensor(6) tensor([ 9,  5,  0,  5,  0, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0]) tensor(6) ['<START>', 'PARAMARIBO', ',', 'Surinam', '1996-08-21', '<STOP>'] 6\n"
     ]
    }
   ],
   "source": [
    "t = next(iter(dataset_conll2003_train_loader))\n",
    "pos = 0\n",
    "print(t[0][pos], sum(t[0][pos]>0), t[1][pos], t[2][pos], t[3][pos], t[4][pos], len(t[4][pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041 3453\n"
     ]
    }
   ],
   "source": [
    "print(dataset_conll2003_train_loader.dataset.__len__(), dataset_conll2003_test_loader.dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## token-emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(global_vectors.get_vecs_by_tokens(START_TAG)[0:10],global_vectors.get_vecs_by_tokens(STOP_TAG)[0:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_weight = global_vectors.get_vecs_by_tokens(list(word_to_ix.keys()))\n",
    "glove_weight[-2], glove_weight[-1] = torch.randn(300), torch.randn(300)\n",
    "embedding_glove = nn.Embedding.from_pretrained(glove_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.4590e-01,  6.4280e-01,  1.2850e+00, -1.9312e+00, -7.7402e-01,\n",
       "         1.1995e+00, -1.1461e+00, -7.8981e-01,  2.0956e-02,  2.0498e+00,\n",
       "        -1.9247e-01,  1.1378e+00,  3.1604e-01,  7.2349e-01,  1.0631e+00,\n",
       "         4.6080e-01, -2.2031e+00, -1.6780e-01, -5.1149e-01,  7.7356e-01,\n",
       "         2.8797e-01, -2.8857e-01, -1.8801e+00, -1.0674e+00,  2.7922e-01,\n",
       "        -4.4782e-01,  2.5344e-01,  3.0045e-02,  3.5837e-01,  2.1979e-01,\n",
       "        -1.1757e+00, -9.0621e-02, -8.6704e-01,  3.3297e-02,  6.8099e-01,\n",
       "         1.1265e-01, -1.2839e+00, -4.6444e-01,  1.3207e+00,  6.8688e-01,\n",
       "        -8.0311e-01, -5.2655e-01, -9.6999e-01,  7.4705e-01,  8.9972e-01,\n",
       "         7.6337e-02, -1.3263e+00, -5.2868e-01, -1.2421e+00,  2.5707e+00,\n",
       "        -4.9449e-01, -7.2044e-02,  1.3960e+00, -5.3946e-01, -3.2282e-01,\n",
       "         7.2201e-01,  1.5926e+00, -3.4144e-01, -1.3687e+00,  4.3110e-01,\n",
       "         1.6970e+00, -3.4245e-01,  8.4286e-01, -1.1194e+00, -9.9454e-01,\n",
       "        -1.2910e+00, -2.3693e+00, -9.6196e-01,  1.8675e+00,  6.5074e-01,\n",
       "         1.4281e+00, -4.8322e-01, -3.7674e-01, -7.4919e-01, -8.4121e-02,\n",
       "         1.4606e+00, -1.7088e-01,  4.4345e-01, -1.8426e-01,  1.0220e+00,\n",
       "        -3.4809e-02, -1.0560e+00, -9.8891e-01,  2.2079e-01,  1.7268e+00,\n",
       "        -8.3595e-01,  2.7411e-02, -4.5270e-01, -1.6870e-02, -6.4009e-01,\n",
       "         7.6923e-01, -1.8251e+00, -3.2601e-01,  3.0802e-01, -1.3564e+00,\n",
       "        -2.4702e-01,  1.0146e+00, -1.8303e+00,  4.4257e-01, -1.5958e+00,\n",
       "        -1.2843e+00, -1.1693e+00,  1.6012e-01,  1.1402e+00,  1.0918e+00,\n",
       "         3.4161e-01,  9.1475e-01, -9.3893e-01,  1.4399e+00,  8.9328e-01,\n",
       "        -3.3349e-01, -9.7543e-01, -5.3579e-01, -4.9637e-01, -2.0549e+00,\n",
       "         1.6560e+00, -1.8083e+00, -1.8136e+00, -4.5958e-01,  9.8126e-01,\n",
       "         1.0224e+00, -1.6682e-01, -5.9131e-01,  3.3705e-01, -1.7611e+00,\n",
       "         1.5632e+00,  2.4318e-01,  3.0899e-01, -1.3720e+00,  1.4514e+00,\n",
       "         1.0851e+00,  2.8280e-01, -7.6391e-01,  7.7553e-01,  1.1547e+00,\n",
       "         2.1312e-01, -1.2535e+00, -8.9438e-01,  3.5810e-01, -1.5241e-01,\n",
       "        -5.9891e-04,  1.8993e-01,  5.2896e-01, -7.0829e-01,  7.1116e-01,\n",
       "        -1.9462e+00, -5.1936e-02,  1.6048e+00,  6.7926e-01, -6.9124e-01,\n",
       "         1.7630e+00,  4.8803e-01, -9.7149e-01, -6.9303e-01,  1.4923e+00,\n",
       "        -2.2266e+00, -2.2985e-01, -7.1046e-01, -2.9571e+00, -7.8567e-01,\n",
       "         8.2260e-01, -1.4598e+00, -6.0097e-01,  7.7590e-01,  5.5937e-01,\n",
       "         1.8493e+00,  1.0183e+00, -1.0204e+00, -5.0911e-02,  8.5431e-01,\n",
       "         1.1865e+00, -1.1105e+00, -2.4923e-02,  3.8748e-02,  4.0563e-01,\n",
       "        -1.4305e+00,  3.4659e-01, -1.1507e+00,  3.5243e-01, -9.1933e-01,\n",
       "         1.6928e+00,  2.2746e+00, -6.7985e-02,  7.5974e-01,  5.3565e-01,\n",
       "         7.2104e-01,  1.4886e-01,  1.6960e-01,  1.0304e+00,  5.2995e-01,\n",
       "        -1.5251e+00,  1.9266e+00, -1.9695e+00,  3.4431e-01,  6.9571e-01,\n",
       "         8.1537e-01,  1.1228e+00, -9.4448e-01, -4.0921e-02, -3.9148e-01,\n",
       "        -5.4308e-01, -2.7395e-01,  6.4685e-01, -1.3422e-01,  2.7936e-01,\n",
       "        -1.8804e+00, -5.4271e-01,  8.9759e-01,  5.7941e-01,  7.4139e-01,\n",
       "        -5.0013e-01, -2.9180e-01, -6.7437e-01, -1.1642e+00, -2.7485e-01,\n",
       "         8.6646e-02,  1.3244e-01, -1.0051e+00,  9.3841e-01,  8.7054e-01,\n",
       "         1.2442e+00, -7.1293e-02, -1.0135e+00, -2.1554e-01, -4.3528e-01,\n",
       "        -3.5822e-01, -2.6841e-01, -1.0027e+00, -3.2147e-01, -5.2180e-01,\n",
       "         1.0178e+00,  7.0035e-01,  2.9113e-01, -1.0971e+00,  9.0869e-01,\n",
       "         7.8946e-02, -1.7191e+00, -5.3524e-01,  4.7191e-01, -2.1420e-01,\n",
       "        -8.9777e-01,  5.0196e-01, -2.2765e-01, -5.5226e-02,  2.1138e-01,\n",
       "        -1.3171e+00, -1.3294e+00,  1.2157e+00,  1.8280e-02, -1.2688e+00,\n",
       "        -4.0750e-01,  1.5497e-01, -1.0999e+00, -8.4141e-01, -3.2663e-01,\n",
       "        -2.0324e+00,  1.5001e-01, -1.9932e-02, -1.8004e-01, -6.0667e-01,\n",
       "         6.5288e-01, -1.5453e+00, -7.1603e-01,  1.4400e-01, -3.2112e-01,\n",
       "         3.9474e-01, -7.6752e-01,  9.2201e-01,  1.5399e+00,  3.2592e-01,\n",
       "        -1.8042e+00, -1.7744e+00,  5.4825e-01, -8.7418e-01,  1.5749e+00,\n",
       "        -3.9910e-01,  1.5077e-01, -8.5671e-01, -1.0176e-01,  1.6075e+00,\n",
       "         1.3236e-01,  8.7858e-01, -1.8371e+00, -4.9409e-01, -7.7611e-01,\n",
       "        -4.8663e-01, -2.1376e-01,  3.4958e-02, -1.6293e+00, -1.9675e+00,\n",
       "         8.7371e-01, -4.9801e-01, -1.4742e-02, -7.5757e-01,  2.5574e-01,\n",
       "        -4.7035e-01, -1.1220e-01,  1.7597e-02, -2.4998e+00, -6.4355e-01])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_glove(torch.tensor(glove_weight.shape[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-glove_weight[0]==embedding_glove(torch.LongTensor([0]))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLove_BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, config=None, embedding_weight = None):\n",
    "        super(GLove_BiLSTM_CRF, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # BiLSTM-model 给 emission 层定义参数\n",
    "        self.embedding_dim = self.config.get('embedding_dim', 300)\n",
    "        self.hidden_dim = self.config.get('hidden_dim', 200)\n",
    "        self.vocab_size = self.config.get('vocab_size', 30289)\n",
    "\n",
    "        if embedding_weight is not None:\n",
    "            self.word_embeds = nn.Embedding.from_pretrained(embedding_weight)\n",
    "        else:\n",
    "            self.word_embeds = nn.Embedding(self.vocab_size, self.embedding_dim)            \n",
    "            \n",
    "        self.target_size = self.config.get('num_tags', 11)\n",
    "        self.num_layers = self.config.get('num_layers',1)\n",
    "        self.batch_size = self.config.get('batch_size',16)\n",
    "        self.bidirectional = True\n",
    "\n",
    "        # lstm\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim//2,\n",
    "                            num_layers=self.num_layers, bidirectional=self.bidirectional)\n",
    "        self.hidden2tag1 = nn.Linear(self.hidden_dim, self.target_size*3)\n",
    "        self.hidden2tag2 = nn.Linear(self.target_size*3, self.target_size)\n",
    "        self.dropout010 = nn.Dropout(0.1)\n",
    "        self.dropout020 = nn.Dropout(0.2)\n",
    "#         self.hidden_init = self.init_hidden()\n",
    "\n",
    "        # CRF-model\n",
    "        self.crf = CRF(self.config.get('num_tags', 9), batch_first=True)\n",
    "        self.crf.transitions.data[ner_tag2id[START_TAG], :] = -10000\n",
    "        self.crf.transitions.data[:, ner_tag2id[STOP_TAG]] = -10000\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers*2 if self.bidirectional else self.num_layers,\n",
    "                             self.batch_size, self.hidden_dim//2)\n",
    "        cell = torch.zeros(self.num_layers*2 if self.bidirectional else self.num_layers,\n",
    "                             self.batch_size, self.hidden_dim//2)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def cal_mask(self, sent_len):\n",
    "        sent_len = sent_len.cuda()\n",
    "        batch_size = torch.tensor(len(sent_len))\n",
    "        batch_size = batch_size.cuda()\n",
    "        batch_seq_len_max = max(sent_len)\n",
    "        mask = torch.zeros((batch_size.item(), batch_seq_len_max))\n",
    "        mask = mask.cuda()\n",
    "        for mask_i in range(mask.shape[0]):\n",
    "            mask[mask_i][0:sent_len[mask_i]] = 1\n",
    "#         assert sum(mask.sum(axis=1) == sent_len) // batch_size == 1\n",
    "        mask = mask > 0\n",
    "        return mask\n",
    "    \n",
    "    def _get_features(self, sent, sent_len):\n",
    "#         mask = self.cal_mask(sent_len)\n",
    "            \n",
    "        embeds = self.word_embeds(sent)\n",
    "        embed_packed = pack_padded_sequence(embeds, lengths=sent_len.to('cpu'),\n",
    "                                            batch_first=True,\n",
    "                                            enforce_sorted=False)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embed_packed) #, self.hidden_init)\n",
    "        lstm_out, lens = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        tag_score = self.hidden2tag1(lstm_out)\n",
    "        tag_score = self.dropout010(tag_score)\n",
    "        tag_score = self.hidden2tag2(tag_score)\n",
    "        return tag_score\n",
    "    \n",
    "    def neg_log_likelihood(self, sent, label, sent_len):\n",
    "        assert sent.shape[0:2]==label.shape\n",
    "        mask = self.cal_mask(sent_len)\n",
    "        mask = mask.cuda()\n",
    "        feats = self._get_features(sent, sent_len)\n",
    "        return self.crf(feats, label, mask)\n",
    "\n",
    "    def forward(self, sent, sent_len):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sent: 输入的已转换为token_id的句子，(batch_len * sent_len * token_emb_len)\n",
    "        :param sent_len: tensor(list(int))\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        feats = self._get_features(sent, sent_len)\n",
    "        return torch.tensor(self.crf.decode(feats))\n",
    "#         batch_size = len(sent_len)\n",
    "#         batch_seq_len_max = max(sent_len)\n",
    "#         mask = torch.zeros((batch_size, batch_seq_len_max))\n",
    "#         for mask_i in range(mask.shape[0]):\n",
    "#             mask[mask_i][0:sent_len[mask_i]] = 1\n",
    "#         assert sum(mask.sum(axis=1) == sent_len) // batch_size == 1\n",
    "#         mask = self.cal_mask(sent_len)\n",
    "            \n",
    "#         embeds = self.word_embeds(sent)\n",
    "#         embed_packed = pack_padded_sequence(embeds, lengths=sent_len.to('cpu'),\n",
    "#                                             batch_first=True,\n",
    "#                                             enforce_sorted=False)\n",
    "#         lstm_out, (hidden, cell) = self.lstm(embed_packed) #, self.hidden_init)\n",
    "#         lstm_out, lens = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "#         tag_score = self.hidden2tag1(lstm_out)\n",
    "#         tag_score = self.dropout010(tag_score)\n",
    "#         tag_score = self.hidden2tag2(tag_score)\n",
    "#         tag_score = self.crf(tag_score)\n",
    "# #         tag_score = nn.functional.softmax(tag_score, dim=-1)\n",
    "#         return tag_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_tags': 11,\n",
       " 'num_layers': 1,\n",
       " 'embedding_dim': 300,\n",
       " 'vocab_size': 30291,\n",
       " 'hidden_dim': 100,\n",
       " 'batch_size': 32,\n",
       " 'START_TAG': '<START>',\n",
       " 'STOP_TAG': '<STOP>'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config = {\n",
    "    'num_tags':len(ner_tag2id),\n",
    "    'num_layers':1,\n",
    "    'embedding_dim':300,\n",
    "    'vocab_size':glove_weight.shape[0],\n",
    "    'hidden_dim':100,\n",
    "    'batch_size':32,\n",
    "    'START_TAG':\"<START>\",\n",
    "    'STOP_TAG': \"<STOP>\"\n",
    "}\n",
    "\n",
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of GLove_BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(30291, 300)\n",
       "  (lstm): LSTM(300, 50, bidirectional=True)\n",
       "  (hidden2tag1): Linear(in_features=100, out_features=33, bias=True)\n",
       "  (hidden2tag2): Linear(in_features=33, out_features=11, bias=True)\n",
       "  (dropout010): Dropout(p=0.1, inplace=False)\n",
       "  (dropout020): Dropout(p=0.2, inplace=False)\n",
       "  (crf): CRF(num_tags=11)\n",
       ")>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'V3-Emb=Glove300-bilstmCRFLayer=1Hidden=200Dropout0.1Batch=32Learn=1e-1'\n",
    "model_lstm_crf = GLove_BiLSTM_CRF(config=Config, embedding_weight=glove_weight)\n",
    "\n",
    "model_lstm_crf = model_lstm_crf.cuda()\n",
    "\n",
    "model_lstm_crf.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 1.8335430366624725), (3, 4.072896341999332), (0, 0.15181673199222645), (7, 7.488300691616573), (10, 1.8335430366624725), (1, 3.900723905723906), (2, 5.685684138201806), (5, 3.605711173358232), (4, 6.950533957283417), (8, 22.28985088985089), (6, 22.251320464803612)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1242,  3.1915,  4.6519,  3.3324,  5.6868,  2.9501, 18.2056,  6.1268,\n",
       "        18.2372,  1.5002,  1.5002])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动计算验证权重\n",
    "import collections\n",
    "ner_tags_all = torch.cat(dataset_conll2003_train['ner_tags_fill'])\n",
    "t=collections.Counter(ner_tags_all.numpy())\n",
    "res = []\n",
    "for k in t:\n",
    "    res.append((k, len(ner_tags_all)/9/t[k]))\n",
    "print(res)\n",
    "\n",
    "class_weights=sklearn.utils.class_weight.compute_class_weight(\n",
    "    class_weight='balanced',classes=np.unique(ner_tags_all),y=ner_tags_all.numpy())\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_lstm_crf.parameters(), lr=1e-1)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean', weight=class_weights.cuda()) \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试model\n",
    "# tmp = next(iter(dataset_conll2003_train_loader))\n",
    "# # print(tmp)\n",
    "# sent = tmp[0]\n",
    "# print(sent.shape)\n",
    "# sent_len = tmp[1]\n",
    "# y = tmp[2]\n",
    "\n",
    "# print(sent.shape, y.shape)\n",
    "\n",
    "# sent = sent.cuda()\n",
    "# sent_len = sent_len.cuda()\n",
    "\n",
    "# model_lstm_crf = model_lstm_crf.cuda()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     res1 = model_lstm_crf(sent.cuda(), sent_len.cuda())\n",
    "#     loss = model_lstm_crf.neg_log_likelihood(sent=sent, label=y, sent_len=sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_single_epoch(model, data_iter=None, optimizer=None, loss_fn=None, is_train=False):\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()   \n",
    "    correct_curr, correct_sum, loss_sum, loss_curr = 0, 0, 0, 0\n",
    "    loss_list, accuracy_list = [], []\n",
    "    print('Total (training) batch = {}'.format(len(data_iter)))\n",
    "    batch_i = 0\n",
    "    data_iter_len = data_iter.dataset.__len__() # total sample num.\n",
    "    batch_num = len(data_iter)\n",
    "    \n",
    "    batch_loss_list = list()\n",
    "    logits_list = list()\n",
    "    y_list, y_len_list = [], []\n",
    "    for batch_data in data_iter:\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_i += 1\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "        x, x_len, y, y_len, _ = batch_data\n",
    "        x = x.cuda()\n",
    "        x_len = x_len.cuda()\n",
    "        y = y.cuda()\n",
    "        # model predict \n",
    "        logits = model(x, x_len)\n",
    "        assert logits.shape[0:2]==x.shape[0:2]\n",
    "        # compute loss \n",
    "#         batch_loss = 0\n",
    "        batch_loss = - model.neg_log_likelihood(sent=x, label=y, sent_len=x_len)\n",
    "#         for i in range(logits.size(0)): # num of samples in one batch\n",
    "#             loss = loss_fn(logits[i], y[i])\n",
    "#             batch_loss += loss \n",
    "        \n",
    "#         batch_loss /= logits.size(0)\n",
    "        if is_train:\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 记录\n",
    "        batch_loss_list.append(batch_loss.item())\n",
    "        logits_list.append(logits)\n",
    "        y_list.append(y)\n",
    "        y_len_list.append(y_len)\n",
    "#         if batch_i%100==0:\n",
    "#             print(batch_loss.item())\n",
    "        \n",
    "        x = x.cpu()\n",
    "        x_len = x_len.cpu()\n",
    "        y = y.cpu()\n",
    "        \n",
    "#     print(sum(batch_loss_list)/batch_num)\n",
    "    return batch_loss_list, logits_list, y_list, y_len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(model, data_iter=None, loss_fn=None):\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    correct_curr, correct_sum, loss_sum, loss_curr = 0, 0, 0, 0\n",
    "    loss_list, accuracy_list = [], []\n",
    "    print('Total (training) batch = {}'.format(len(data_iter)))\n",
    "    batch_i = 0\n",
    "    data_iter_len = data_iter.dataset.__len__() # total sample num.\n",
    "    batch_num = len(data_iter)\n",
    "    \n",
    "    batch_loss_list = list()\n",
    "    logits_list = list()\n",
    "    y_list, y_len_list = [], []\n",
    "    for batch_data in data_iter:\n",
    "        batch_i += 1\n",
    "        x, x_len, y, y_len,_ = batch_data\n",
    "        x = x.cuda()\n",
    "        x_len = x_len.cuda()\n",
    "        y = y.cuda()\n",
    "        # model predict \n",
    "        logits = model(x, x_len)\n",
    "        assert logits.shape[0:2]==x.shape[0:2]\n",
    "        # compute loss \n",
    "        batch_loss = - model.neg_log_likelihood(sent=x, label=y, sent_len=x_len)\n",
    "#         batch_loss = 0\n",
    "#         for i in range(logits.size(0)): # num of samples in one batch\n",
    "#             loss = loss_fn(logits[i], y[i])\n",
    "#             batch_loss -= loss \n",
    "        \n",
    "#         batch_loss /= logits.size(0)\n",
    "\n",
    "        # 记录\n",
    "        batch_loss_list.append(batch_loss.item())\n",
    "        logits_list.append(logits)\n",
    "        y_list.append(y)\n",
    "        y_len_list.append(y_len)\n",
    "#         if batch_i%100==0:\n",
    "#             print(batch_loss.item())\n",
    "        \n",
    "        x = x.cpu()\n",
    "        x_len = x_len.cpu()\n",
    "        y = y.cpu()\n",
    "        \n",
    "#     print(sum(batch_loss_list)/batch_num)\n",
    "    return batch_loss_list, logits_list, y_list, y_len_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyFunctions3 import MyTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Epoch = 0\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 37084.86910149\n",
      "Total (training) batch = 108\n",
      "testing loss = 188.79130554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/web_server/antispam/project/kml_public/liuguizhou/my_train/POS/conll2003/MyFunctions3.py:25: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  assert sum(mask.sum(axis=1) == y_len_list[logit_i]) // batch_len == 1\n",
      "  2%|▏         | 1/50 [01:03<51:31, 63.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 51.67%, precision = 75.46%, accuracy = 89.11%, f1 = 61.34%\n",
      "Test metrics\n",
      "recall = 63.24%, precision = 87.93%, accuracy = 92.06%, f1 = 73.57%\n",
      "==================================================\n",
      "Epoch = 1\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 242.62431958\n",
      "Total (training) batch = 108\n",
      "testing loss = 172.03254982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [02:05<50:00, 62.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 64.89%, precision = 86.60%, accuracy = 92.45%, f1 = 74.19%\n",
      "Test metrics\n",
      "recall = 69.51%, precision = 85.76%, accuracy = 92.66%, f1 = 76.79%\n",
      "==================================================\n",
      "Epoch = 2\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 139.09085326\n",
      "Total (training) batch = 108\n",
      "testing loss = 148.62897265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [03:07<48:47, 62.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 69.44%, precision = 89.01%, accuracy = 93.46%, f1 = 78.01%\n",
      "Test metrics\n",
      "recall = 71.34%, precision = 87.62%, accuracy = 93.23%, f1 = 78.64%\n",
      "==================================================\n",
      "Epoch = 3\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 108.40065265\n",
      "Total (training) batch = 108\n",
      "testing loss = 99.52369344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [04:08<47:31, 61.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 74.61%, precision = 91.47%, accuracy = 94.59%, f1 = 82.18%\n",
      "Test metrics\n",
      "recall = 76.61%, precision = 84.21%, accuracy = 93.41%, f1 = 80.23%\n",
      "==================================================\n",
      "Epoch = 4\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 97.85171910\n",
      "Total (training) batch = 108\n",
      "testing loss = 149.86701852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [05:11<46:34, 62.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 76.45%, precision = 92.56%, accuracy = 95.04%, f1 = 83.74%\n",
      "Test metrics\n",
      "recall = 69.19%, precision = 93.88%, accuracy = 93.83%, f1 = 79.67%\n",
      "==================================================\n",
      "Epoch = 5\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 82.01887567\n",
      "Total (training) batch = 108\n",
      "testing loss = 109.33852860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [06:13<45:31, 62.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 78.79%, precision = 93.42%, accuracy = 95.53%, f1 = 85.48%\n",
      "Test metrics\n",
      "recall = 76.08%, precision = 89.98%, accuracy = 94.34%, f1 = 82.45%\n",
      "==================================================\n",
      "Epoch = 6\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 74.18475400\n",
      "Total (training) batch = 108\n",
      "testing loss = 109.29346212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [07:15<44:39, 62.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 80.01%, precision = 93.99%, accuracy = 95.80%, f1 = 86.44%\n",
      "Test metrics\n",
      "recall = 77.66%, precision = 90.52%, accuracy = 94.68%, f1 = 83.60%\n",
      "==================================================\n",
      "Epoch = 7\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 67.21776863\n",
      "Total (training) batch = 108\n",
      "testing loss = 130.36788785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [08:20<44:07, 63.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 81.57%, precision = 94.45%, accuracy = 96.12%, f1 = 87.54%\n",
      "Test metrics\n",
      "recall = 73.84%, precision = 90.95%, accuracy = 94.15%, f1 = 81.51%\n",
      "==================================================\n",
      "Epoch = 8\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 75.94999733\n",
      "Total (training) batch = 108\n",
      "testing loss = 79.29000586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [09:22<42:51, 62.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 81.90%, precision = 94.26%, accuracy = 96.14%, f1 = 87.65%\n",
      "Test metrics\n",
      "recall = 82.04%, precision = 92.75%, accuracy = 95.74%, f1 = 87.07%\n",
      "==================================================\n",
      "Epoch = 9\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 64.29180152\n",
      "Total (training) batch = 108\n",
      "testing loss = 111.27881481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [10:24<41:39, 62.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 83.54%, precision = 95.06%, accuracy = 96.52%, f1 = 88.93%\n",
      "Test metrics\n",
      "recall = 83.75%, precision = 91.40%, accuracy = 95.79%, f1 = 87.41%\n",
      "==================================================\n",
      "Epoch = 10\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 52.15684741\n",
      "Total (training) batch = 108\n",
      "testing loss = 83.05177830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [11:27<40:44, 62.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 84.93%, precision = 95.44%, accuracy = 96.80%, f1 = 89.88%\n",
      "Test metrics\n",
      "recall = 81.57%, precision = 92.21%, accuracy = 95.58%, f1 = 86.56%\n",
      "==================================================\n",
      "Epoch = 11\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 52.36460851\n",
      "Total (training) batch = 108\n",
      "testing loss = 107.19123134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [12:29<39:36, 62.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 85.32%, precision = 95.57%, accuracy = 96.88%, f1 = 90.15%\n",
      "Test metrics\n",
      "recall = 79.73%, precision = 92.88%, accuracy = 95.39%, f1 = 85.81%\n",
      "==================================================\n",
      "Epoch = 12\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 46.55162236\n",
      "Total (training) batch = 108\n",
      "testing loss = 97.47117897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [13:34<38:52, 63.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 86.09%, precision = 96.00%, accuracy = 97.07%, f1 = 90.78%\n",
      "Test metrics\n",
      "recall = 81.76%, precision = 88.94%, accuracy = 95.04%, f1 = 85.19%\n",
      "==================================================\n",
      "Epoch = 13\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 44.85513081\n",
      "Total (training) batch = 108\n",
      "testing loss = 82.78378963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [14:36<37:41, 62.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 86.94%, precision = 96.13%, accuracy = 97.23%, f1 = 91.31%\n",
      "Test metrics\n",
      "recall = 83.39%, precision = 92.73%, accuracy = 95.96%, f1 = 87.82%\n",
      "==================================================\n",
      "Epoch = 14\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 49.92351771\n",
      "Total (training) batch = 108\n",
      "testing loss = 84.62487736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [15:37<36:25, 62.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 87.28%, precision = 96.21%, accuracy = 97.30%, f1 = 91.52%\n",
      "Test metrics\n",
      "recall = 81.98%, precision = 92.76%, accuracy = 95.73%, f1 = 87.04%\n",
      "==================================================\n",
      "Epoch = 15\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 45.50068344\n",
      "Total (training) batch = 108\n",
      "testing loss = 116.48005521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [16:39<35:18, 62.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 87.37%, precision = 96.22%, accuracy = 97.31%, f1 = 91.58%\n",
      "Test metrics\n",
      "recall = 83.52%, precision = 92.96%, accuracy = 96.02%, f1 = 87.99%\n",
      "==================================================\n",
      "Epoch = 16\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 39.47901686\n",
      "Total (training) batch = 108\n",
      "testing loss = 75.01201271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [17:44<34:35, 62.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 89.19%, precision = 96.92%, accuracy = 97.72%, f1 = 92.89%\n",
      "Test metrics\n",
      "recall = 82.78%, precision = 92.62%, accuracy = 95.84%, f1 = 87.42%\n",
      "==================================================\n",
      "Epoch = 17\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 36.74634117\n",
      "Total (training) batch = 108\n",
      "testing loss = 124.80183241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [18:47<33:33, 62.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 89.75%, precision = 97.08%, accuracy = 97.83%, f1 = 93.27%\n",
      "Test metrics\n",
      "recall = 83.23%, precision = 88.15%, accuracy = 95.12%, f1 = 85.62%\n",
      "==================================================\n",
      "Epoch = 18\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 54.13782196\n",
      "Total (training) batch = 108\n",
      "testing loss = 86.39618909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [19:49<32:27, 62.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 89.31%, precision = 96.57%, accuracy = 97.68%, f1 = 92.80%\n",
      "Test metrics\n",
      "recall = 81.67%, precision = 95.08%, accuracy = 96.06%, f1 = 87.86%\n",
      "==================================================\n",
      "Epoch = 19\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 32.45031571\n",
      "Total (training) batch = 108\n",
      "testing loss = 82.14926430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [20:52<31:21, 62.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.55%, precision = 97.21%, accuracy = 97.99%, f1 = 93.76%\n",
      "Test metrics\n",
      "recall = 81.56%, precision = 92.99%, accuracy = 95.70%, f1 = 86.90%\n",
      "==================================================\n",
      "Epoch = 20\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 34.24285932\n",
      "Total (training) batch = 108\n",
      "testing loss = 88.36972781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [21:53<30:08, 62.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.50%, precision = 97.40%, accuracy = 98.01%, f1 = 93.82%\n",
      "Test metrics\n",
      "recall = 82.24%, precision = 93.81%, accuracy = 95.95%, f1 = 87.64%\n",
      "==================================================\n",
      "Epoch = 21\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 29.27049147\n",
      "Total (training) batch = 108\n",
      "testing loss = 84.29703126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [22:55<28:57, 62.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.95%, precision = 97.36%, accuracy = 98.07%, f1 = 94.04%\n",
      "Test metrics\n",
      "recall = 81.39%, precision = 93.74%, accuracy = 95.80%, f1 = 87.13%\n",
      "==================================================\n",
      "Epoch = 22\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 33.24487381\n",
      "Total (training) batch = 108\n",
      "testing loss = 81.37540754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [23:57<27:57, 62.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.39%, precision = 97.30%, accuracy = 97.97%, f1 = 93.72%\n",
      "Test metrics\n",
      "recall = 83.75%, precision = 93.84%, accuracy = 96.20%, f1 = 88.51%\n",
      "==================================================\n",
      "Epoch = 23\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 30.55850711\n",
      "Total (training) batch = 108\n",
      "testing loss = 84.23894382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [25:00<27:02, 62.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.93%, precision = 97.43%, accuracy = 98.08%, f1 = 94.07%\n",
      "Test metrics\n",
      "recall = 83.46%, precision = 94.05%, accuracy = 96.19%, f1 = 88.44%\n",
      "==================================================\n",
      "Epoch = 24\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 36.25971231\n",
      "Total (training) batch = 108\n",
      "testing loss = 92.52253070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [26:05<26:21, 63.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.73%, precision = 97.33%, accuracy = 98.03%, f1 = 93.91%\n",
      "Test metrics\n",
      "recall = 82.57%, precision = 94.38%, accuracy = 96.10%, f1 = 88.08%\n",
      "==================================================\n",
      "Epoch = 25\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 35.13052602\n",
      "Total (training) batch = 108\n",
      "testing loss = 92.84078623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [27:12<25:41, 64.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.47%, precision = 97.11%, accuracy = 97.96%, f1 = 93.67%\n",
      "Test metrics\n",
      "recall = 83.16%, precision = 93.18%, accuracy = 95.99%, f1 = 87.88%\n",
      "==================================================\n",
      "Epoch = 26\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 38.26826751\n",
      "Total (training) batch = 108\n",
      "testing loss = 96.23392437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [28:14<24:21, 63.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.48%, precision = 97.11%, accuracy = 97.96%, f1 = 93.67%\n",
      "Test metrics\n",
      "recall = 84.74%, precision = 93.01%, accuracy = 96.22%, f1 = 88.68%\n",
      "==================================================\n",
      "Epoch = 27\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 35.66216505\n",
      "Total (training) batch = 108\n",
      "testing loss = 111.90232782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [29:15<23:04, 62.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.91%, precision = 97.32%, accuracy = 98.06%, f1 = 94.01%\n",
      "Test metrics\n",
      "recall = 82.63%, precision = 90.99%, accuracy = 95.54%, f1 = 86.61%\n",
      "==================================================\n",
      "Epoch = 28\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 40.51158003\n",
      "Total (training) batch = 108\n",
      "testing loss = 101.69696670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [30:24<22:41, 64.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 90.02%, precision = 97.17%, accuracy = 97.89%, f1 = 93.46%\n",
      "Test metrics\n",
      "recall = 84.63%, precision = 92.76%, accuracy = 96.16%, f1 = 88.51%\n",
      "==================================================\n",
      "Epoch = 29\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 51.86252762\n",
      "Total (training) batch = 108\n",
      "testing loss = 106.79787685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [31:29<21:34, 64.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 89.17%, precision = 96.83%, accuracy = 97.70%, f1 = 92.84%\n",
      "Test metrics\n",
      "recall = 83.20%, precision = 93.88%, accuracy = 96.12%, f1 = 88.22%\n",
      "==================================================\n",
      "Epoch = 30\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 53.15311672\n",
      "Total (training) batch = 108\n",
      "testing loss = 214.90400413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [32:30<20:11, 63.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 88.70%, precision = 96.62%, accuracy = 97.59%, f1 = 92.49%\n",
      "Test metrics\n",
      "recall = 74.32%, precision = 88.52%, accuracy = 93.83%, f1 = 80.80%\n",
      "==================================================\n",
      "Epoch = 31\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 74.59288999\n",
      "Total (training) batch = 108\n",
      "testing loss = 110.22462859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [33:36<19:15, 64.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 86.98%, precision = 95.94%, accuracy = 97.21%, f1 = 91.24%\n",
      "Test metrics\n",
      "recall = 85.01%, precision = 92.28%, accuracy = 96.14%, f1 = 88.50%\n",
      "==================================================\n",
      "Epoch = 32\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 73.33183315\n",
      "Total (training) batch = 108\n",
      "testing loss = 110.90926107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [34:40<18:12, 64.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 87.08%, precision = 95.95%, accuracy = 97.22%, f1 = 91.30%\n",
      "Test metrics\n",
      "recall = 83.84%, precision = 93.02%, accuracy = 96.08%, f1 = 88.19%\n",
      "==================================================\n",
      "Epoch = 33\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 86.17208862\n",
      "Total (training) batch = 108\n",
      "testing loss = 124.46328170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [35:42<16:55, 63.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 85.59%, precision = 95.76%, accuracy = 96.96%, f1 = 90.39%\n",
      "Test metrics\n",
      "recall = 79.88%, precision = 89.53%, accuracy = 94.85%, f1 = 84.43%\n",
      "==================================================\n",
      "Epoch = 34\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 113.35994753\n",
      "Total (training) batch = 108\n",
      "testing loss = 184.73359285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [36:43<15:41, 62.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 82.33%, precision = 94.21%, accuracy = 96.20%, f1 = 87.87%\n",
      "Test metrics\n",
      "recall = 72.21%, precision = 96.36%, accuracy = 94.67%, f1 = 82.56%\n",
      "==================================================\n",
      "Epoch = 35\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 130.14356748\n",
      "Total (training) batch = 108\n",
      "testing loss = 106.75198025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [37:44<14:31, 62.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 81.12%, precision = 94.00%, accuracy = 95.98%, f1 = 87.08%\n",
      "Test metrics\n",
      "recall = 83.57%, precision = 90.60%, accuracy = 95.62%, f1 = 86.94%\n",
      "==================================================\n",
      "Epoch = 36\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 159.04222131\n",
      "Total (training) batch = 108\n",
      "testing loss = 109.75202546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [38:45<13:25, 61.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 78.72%, precision = 92.39%, accuracy = 95.36%, f1 = 85.01%\n",
      "Test metrics\n",
      "recall = 81.27%, precision = 91.54%, accuracy = 95.42%, f1 = 86.10%\n",
      "==================================================\n",
      "Epoch = 37\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 204.22103791\n",
      "Total (training) batch = 108\n",
      "testing loss = 145.98527922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [39:48<12:28, 62.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 72.66%, precision = 88.59%, accuracy = 93.87%, f1 = 79.84%\n",
      "Test metrics\n",
      "recall = 71.35%, precision = 95.57%, accuracy = 94.42%, f1 = 81.71%\n",
      "==================================================\n",
      "Epoch = 38\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 748.48452404\n",
      "Total (training) batch = 108\n",
      "testing loss = 345.27642144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [40:50<11:21, 62.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 59.80%, precision = 78.66%, accuracy = 90.57%, f1 = 67.94%\n",
      "Test metrics\n",
      "recall = 54.39%, precision = 80.97%, accuracy = 89.80%, f1 = 65.07%\n",
      "==================================================\n",
      "Epoch = 39\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 354.66648945\n",
      "Total (training) batch = 108\n",
      "testing loss = 150.93711118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [41:51<10:18, 61.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 66.50%, precision = 83.14%, accuracy = 92.14%, f1 = 73.89%\n",
      "Test metrics\n",
      "recall = 72.36%, precision = 87.74%, accuracy = 93.41%, f1 = 79.31%\n",
      "==================================================\n",
      "Epoch = 40\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 1100.56265794\n",
      "Total (training) batch = 108\n",
      "testing loss = 755.62313730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [42:52<09:14, 61.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 47.48%, precision = 63.37%, accuracy = 86.63%, f1 = 54.28%\n",
      "Test metrics\n",
      "recall = 44.00%, precision = 82.63%, accuracy = 88.60%, f1 = 57.42%\n",
      "==================================================\n",
      "Epoch = 41\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 1258.16356421\n",
      "Total (training) batch = 108\n",
      "testing loss = 693.52860966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [43:54<08:12, 61.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 35.65%, precision = 47.43%, accuracy = 82.64%, f1 = 40.71%\n",
      "Test metrics\n",
      "recall = 31.39%, precision = 78.56%, accuracy = 86.52%, f1 = 44.85%\n",
      "==================================================\n",
      "Epoch = 42\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 1689.06511158\n",
      "Total (training) batch = 108\n",
      "testing loss = 610.34832538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [44:55<07:09, 61.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 24.65%, precision = 33.48%, accuracy = 79.21%, f1 = 28.40%\n",
      "Test metrics\n",
      "recall = 35.31%, precision = 58.15%, accuracy = 84.26%, f1 = 43.94%\n",
      "==================================================\n",
      "Epoch = 43\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 2313.17279595\n",
      "Total (training) batch = 108\n",
      "testing loss = 1974.38868996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [45:56<06:08, 61.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 15.55%, precision = 20.94%, accuracy = 76.06%, f1 = 17.85%\n",
      "Test metrics\n",
      "recall = 12.15%, precision = 17.49%, accuracy = 74.63%, f1 = 14.34%\n",
      "==================================================\n",
      "Epoch = 44\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 2407.22969462\n",
      "Total (training) batch = 108\n",
      "testing loss = 1187.68603516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [46:57<05:06, 61.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 10.82%, precision = 14.54%, accuracy = 74.46%, f1 = 12.40%\n",
      "Test metrics\n",
      "recall = 8.58%, precision = 57.10%, accuracy = 82.90%, f1 = 14.92%\n",
      "==================================================\n",
      "Epoch = 45\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 3011.38541073\n",
      "Total (training) batch = 108\n",
      "testing loss = 330.11986400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [47:59<04:05, 61.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 6.39%, precision = 9.06%, accuracy = 73.62%, f1 = 7.50%\n",
      "Test metrics\n",
      "recall = 14.34%, precision = 29.35%, accuracy = 79.01%, f1 = 19.26%\n",
      "==================================================\n",
      "Epoch = 46\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 3143.61155947\n",
      "Total (training) batch = 108\n",
      "testing loss = 844.59805411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [49:00<03:03, 61.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 5.44%, precision = 7.41%, accuracy = 72.82%, f1 = 6.27%\n",
      "Test metrics\n",
      "recall = 7.96%, precision = 17.43%, accuracy = 77.33%, f1 = 10.93%\n",
      "==================================================\n",
      "Epoch = 47\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 3337.45968287\n",
      "Total (training) batch = 108\n",
      "testing loss = 289.62915943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [50:03<02:03, 61.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 4.90%, precision = 7.95%, accuracy = 74.62%, f1 = 6.06%\n",
      "Test metrics\n",
      "recall = 7.27%, precision = 33.91%, accuracy = 81.32%, f1 = 11.98%\n",
      "==================================================\n",
      "Epoch = 48\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 2786.42918921\n",
      "Total (training) batch = 108\n",
      "testing loss = 292.75405093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [51:04<01:01, 61.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 4.51%, precision = 11.12%, accuracy = 78.01%, f1 = 6.42%\n",
      "Test metrics\n",
      "recall = 9.95%, precision = 30.38%, accuracy = 80.29%, f1 = 14.99%\n",
      "==================================================\n",
      "Epoch = 49\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 2992.71072395\n",
      "Total (training) batch = 108\n",
      "testing loss = 268.07975260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [52:06<00:00, 62.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "recall = 3.98%, precision = 9.50%, accuracy = 77.61%, f1 = 5.61%\n",
      "Test metrics\n",
      "recall = 2.55%, precision = 34.73%, accuracy = 82.14%, f1 = 4.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f1_test_curr, f1_test_max = 0, 0\n",
    "\n",
    "for epoch in tqdm(range(50)):\n",
    "    print('='*50)\n",
    "    print('Epoch = {}'.format(epoch))\n",
    "    print('='*50)\n",
    "    batch_loss_list, logits_list, y_list, y_len_list = train_eval_single_epoch(model_lstm_crf, \n",
    "                                                                               dataset_conll2003_train_loader, \n",
    "                                                                               optimizer=optimizer,\n",
    "                                                                               loss_fn=loss_fn,\n",
    "                                                                               is_train=True)\n",
    "    scheduler.step() # 加上后好一些\n",
    "    print('training loss = {:.8f}'.format(sum(batch_loss_list)/len(batch_loss_list)))\n",
    "    test_batch_loss_list, test_logits_list, test_y_list, test_y_len_list = func_eval(model_lstm_crf, \n",
    "                                                                               dataset_conll2003_test_loader, \n",
    "                                                                               loss_fn=loss_fn)\n",
    "    print('testing loss = {:.8f}'.format(sum(test_batch_loss_list)/len(test_batch_loss_list)))\n",
    "    \n",
    "#     评估\n",
    "    train_dict = MyTools.func_cal_accu_recall(logits_list=logits_list, y_list=y_list, y_len_list=y_len_list, \n",
    "                                              is_logits_tag=True)\n",
    "    test_dict = MyTools.func_cal_accu_recall(logits_list=test_logits_list, y_list=test_y_list, \n",
    "                                             y_len_list=test_y_len_list, is_logits_tag=True)\n",
    "    \n",
    "    print('Train metrics')\n",
    "    train_metric = MyTools.func_cal_metrics(train_dict)\n",
    "    print('Test metrics')\n",
    "    test_metric = MyTools.func_cal_metrics(test_dict)\n",
    "    \n",
    "    # save model if current f1 rate is better than previous ones\n",
    "    recall_test_curr, precision_test_curr, accuracy_test_curr, f1_test_curr = test_metric\n",
    "    if f1_test_curr > f1_test_max:\n",
    "        torch.save(model_lstm_crf.state_dict(), './models/model_v3_'+model_name+'epoch='+str(epoch)+\n",
    "                   'F1='+str(round(f1_test_curr,4))+\n",
    "                   'accu='+str(round(accuracy_test_curr,4))+\n",
    "                   'recall='+str(round(recall_test_curr,4))+\n",
    "                   'precision='+str(round(precision_test_curr,4)))\n",
    "    f1_test_max = max(f1_test_curr, f1_test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练总结\n",
    "\n",
    "不太清楚为何到后来，训练集开始出现recall和precision的波动；最后下降到10%以下（就很离谱？！）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载及结果探查\n",
    "\n",
    "我们使用第8个epoch模型，\n",
    "- test： recall = 82.04%, precision = 92.75%, accuracy = 95.74%, f1 = 87.07%\n",
    "- eval： recall = 83.20%, precision = 96.40%, accuracy = 96.67%, f1 = 89.32%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load = GLove_BiLSTM_CRF(config=Config)\n",
    "model_load.load_state_dict(torch.load('./models/model_v3_V3-Emb=Glove300-bilstmCRFLayer=1Hidden=200Dropout0.1Batch=32Learn=1e-1epoch=8F1=0.8707accu=0.9574recall=0.8204precision=0.9275'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLove_BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(30291, 300)\n",
       "  (lstm): LSTM(300, 50, bidirectional=True)\n",
       "  (hidden2tag1): Linear(in_features=100, out_features=33, bias=True)\n",
       "  (hidden2tag2): Linear(in_features=33, out_features=11, bias=True)\n",
       "  (dropout010): Dropout(p=0.1, inplace=False)\n",
       "  (dropout020): Dropout(p=0.2, inplace=False)\n",
       "  (crf): CRF(num_tags=11)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (training) batch = 108\n",
      "testing loss = 79.28989290\n",
      "recall = 82.04%, precision = 92.75%, accuracy = 95.74%, f1 = 87.07%\n"
     ]
    }
   ],
   "source": [
    "test_batch_loss_list_load, test_logits_list_load, test_y_list_load, test_y_len_list_load = func_eval(model_load, \n",
    "                                                                           dataset_conll2003_test_loader, \n",
    "                                                                           loss_fn=loss_fn)\n",
    "print('testing loss = {:.8f}'.format(sum(test_batch_loss_list_load)/len(test_batch_loss_list_load)))\n",
    "\n",
    "#     评估\n",
    "test_dict_load = MyTools.func_cal_accu_recall(logits_list=test_logits_list_load, y_list=test_y_list_load, \n",
    "                                         y_len_list=test_y_len_list_load, is_logits_tag=True)\n",
    "test_metric_load = MyTools.func_cal_metrics(test_dict_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (training) batch = 102\n",
      "evaling loss = 66.94167350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/web_server/antispam/project/kml_public/liuguizhou/my_train/POS/conll2003/MyFunctions3.py:25: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  assert sum(mask.sum(axis=1) == y_len_list[logit_i]) // batch_len == 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall = 83.20%, precision = 96.40%, accuracy = 96.67%, f1 = 89.32%\n"
     ]
    }
   ],
   "source": [
    "eval_batch_loss_list_load, eval_logits_list_load, eval_y_list_load, eval_y_len_list_load = func_eval(model_load, \n",
    "                                                                           dataset_conll2003_val_loader, \n",
    "                                                                           loss_fn=loss_fn)\n",
    "print('evaling loss = {:.8f}'.format(sum(eval_batch_loss_list_load)/len(eval_batch_loss_list_load)))\n",
    "\n",
    "#     评估\n",
    "eval_dict_load = MyTools.func_cal_accu_recall(logits_list=eval_logits_list_load, y_list=eval_y_list_load, \n",
    "                                         y_len_list=eval_y_len_list_load, is_logits_tag=True)\n",
    "eval_metric_load = MyTools.func_cal_metrics(eval_dict_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tp': 6655.0,\n",
       " 'tn': 37803.0,\n",
       " 'fp': 520.0,\n",
       " 'fn': 500.0,\n",
       " 'others': 957.0,\n",
       " 'n_total': 46435.0}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[30289,  1831, 30290,  ...,     0,     0,     0],\n",
      "        [30289,  1910,   389,  ...,     0,     0,     0],\n",
      "        [30289,  2342,   318,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [30289,   269, 11672,  ...,     0,     0,     0],\n",
      "        [30289,  1428,  3760,  ...,     0,     0,     0],\n",
      "        [30289,  2260,  5889,  ...,     0,     0,     0]]), tensor([ 3, 11, 10, 25,  5, 52,  9,  9,  4,  4,  9, 28,  7,  9, 33, 10,  9, 16,\n",
      "        37, 43, 36, 12,  4, 27,  9, 23, 10,  8, 15, 43,  6, 22]), tensor([[ 9,  5, 10,  ...,  0,  0,  0],\n",
      "        [ 9,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 9,  3,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 9,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 9,  3,  0,  ...,  0,  0,  0],\n",
      "        [ 9,  5,  6,  ...,  0,  0,  0]]), tensor([ 3, 11, 10, 25,  5, 52,  9,  9,  4,  4,  9, 28,  7,  9, 33, 10,  9, 16,\n",
      "        37, 43, 36, 12,  4, 27,  9, 23, 10,  8, 15, 43,  6, 22]), (['<START>', 'Australia', '<STOP>'], ['<START>', 'Scorers', ':', 'Shkvyrin', 'Igor', '78', ',', 'Shatskikh', 'Oleg', '90', '<STOP>'], ['<START>', 'Everton', '16', '6', '6', '4', '25', '20', '24', '<STOP>'], ['<START>', 'In', 'another', 'attack', ',', 'also', 'on', 'the', 'province', \"'s\", 'south', 'coast', 'on', 'Thursday', 'night', ',', 'two', 'men', 'were', 'shot', 'dead', 'near', 'Umkomaas', '.', '<STOP>'], ['<START>', 'CLEVELAND', 'AT', 'DETROIT', '<STOP>'], ['<START>', '\"', 'If', 'they', 'ca', \"n't\", 'move', 'because', 'they', 'are', 'too', 'weak', ',', 'then', 'we', 'will', 'probably', 'consider', 'very', 'seriously', 'using', 'air', 'delivery', 'means', '(', 'airdrops', ')', '...It', \"'s\", 'complex', ',', 'it', \"'s\", 'dangerous', 'for', 'the', 'air', 'crew', 'that', 'fly', 'in', 'there', 'and', 'it', 'will', 'have', 'to', 'be', 'absolutely', 'necessary', '.', '<STOP>'], ['<START>', 'It', 'has', 'produced', '1.5', 'million', 'hectolitres', '.', '<STOP>'], ['<START>', '7.', 'Madlen', 'Brigger-Summermatter', '(', 'Switzerland', ')', '1:18.23', '<STOP>'], ['<START>', 'Robert', 'Galvin', '<STOP>'], ['<START>', 'Attendance', '33,000', '<STOP>'], ['<START>', 'Kuwait', '-', 'Jassem', 'Al-Huwaidi', '9', ',', '44', '<STOP>'], ['<START>', 'More', 'than', '21,000', 'people', 'have', 'been', 'killed', 'in', 'the', '12-year-old', 'conflict', 'between', 'Turkish', 'security', 'forces', 'and', 'the', 'PKK', ',', 'fighting', 'for', 'Kurdish', 'autonomy', 'or', 'independence', '.', '<STOP>'], ['<START>', 'But', '2', '27/11/96', '5,000', 'Burma', '<STOP>'], ['<START>', '17.', 'Bibiana', 'Perez', '(', 'Italy', ')', '30', '<STOP>'], ['<START>', 'A', 'Santa', 'Claus', 'distributing', 'presents', 'to', 'workers', 'in', 'a', 'German', 'bank', 'on', 'Friday', 'nearly', 'ended', 'up', 'behind', 'bars', 'when', 'a', 'passing', 'police', 'patrol', 'thought', 'he', 'was', 'a', 'robber', 'in', 'disguise', '.', '<STOP>'], ['<START>', 'Torquay', '22', '8', '4', '10', '22', '24', '28', '<STOP>'], ['<START>', '8=', 'Carole', 'Montillet', '(', 'France', ')', '146', '<STOP>'], ['<START>', '\"', 'At', 'the', 'moment', 'there', 'is', 'no', 'evidence', 'the', 'two', 'cases', 'are', 'linked', '.', '<STOP>'], ['<START>', 'League', 'duties', 'restricted', 'the', 'Barbarians', \"'\", 'selectorial', 'options', 'but', 'they', 'still', 'boast', '13', 'internationals', 'including', 'England', 'full-back', 'Tim', 'Stimpson', 'and', 'recalled', 'wing', 'Tony', 'Underwood', ',', 'plus', 'All', 'Black', 'forwards', 'Ian', 'Jones', 'and', 'Norm', 'Hewitt', '.', '<STOP>'], ['<START>', 'is', 'a', 'clear', 'signal', 'that', 'one', 'key', 'of', 'the', 'lines', 'of', 'foreign', 'policy', 'will', 'be', 'the', 'strengthening', 'of', 'the', 'trans-Atlantic', 'cooperation', ',', 'a', 'creation', 'of', 'strategic', 'partnership', 'between', 'Europe', 'and', 'the', 'US', ',', '\"', 'Foreign', 'Minister', 'Josef', 'Zieleniec', 'told', 'Reuters', '.', '<STOP>'], ['<START>', 'K.J.', 'Matthew', 'said', 'at', 'the', 'Asia', 'Rubber', 'Markets', 'meeting', 'here', 'Indian', 'production', 'of', 'natural', 'rubber', 'in', '1996/97', 'will', 'reach', '547,000', 'tonnes', 'against', 'projected', 'demand', 'of', '578,000', 'tonnes', ',', 'a', 'gap', 'of', '31,000', 'tonnes', '.', '<STOP>'], ['<START>', 'Cricket', '-', 'Pakistan', 'beat', 'New', 'Zealand', 'by', '46', 'runs', '.', '<STOP>'], ['<START>', 'AMARILLO', '1996-12-06', '<STOP>'], ['<START>', 'But', 'the', 'students', 'stressed', 'their', 'protests', 'were', 'non-political', 'and', 'they', 'had', 'no', 'contact', 'with', 'Suu', 'Kyi', \"'s\", 'National', 'League', 'for', 'Democracy', '(', 'NLD', ')', '.', '<STOP>'], ['<START>', 'Total', '730.0', '9176.1', '8188.2', '164.2', '3156.0', '2686.7', '<STOP>'], ['<START>', 'Guilin', 'is', 'well', 'known', 'for', 'its', 'mountain', 'and', 'river', 'scenery', 'and', 'is', 'one', 'of', 'China', \"'s\", 'most', 'popular', 'tourist', 'destinations', '.', '<STOP>'], ['<START>', 'Ipswich', '22', '6', '8', '8', '27', '32', '26', '<STOP>'], ['<START>', '210', 'Brett', 'Liddle', '75', '65', '70', '<STOP>'], ['<START>', 'Today', ',', 'Rottweilers', 'are', 'on', 'the', 'way', 'up', ',', '\"', 'Rieck', 'said', '.', '<STOP>'], ['<START>', 'A', 'five-year-old', 'girl', 'in', 'the', 'east', 'China', 'city', 'of', 'Tianjin', 'choked', 'and', 'almost', 'died', 'from', 'cigarette', 'smoke', 'at', 'her', 'grandfather', \"'s\", 'birthday', 'with', 'relatives', 'smoking', 'for', 'hours', 'in', 'a', 'small', 'room', ',', 'the', 'Wen', 'Hui', 'Bao', 'newspaper', 'said', 'on', 'Friday', '.', '<STOP>'], ['<START>', 'Bath', '35', 'Harlequins', '20', '<STOP>'], ['<START>', 'White', 'House', 'spokesman', 'Mike', 'McCurry', 'said', 'Clinton', '\"', 'plans', 'to', 'have', 'regular', 'news', 'conferences', '\"', 'during', 'his', 'second', 'term', '.', '<STOP>']))\n"
     ]
    }
   ],
   "source": [
    "t=next(iter(dataset_conll2003_test_loader))\n",
    "# print(t)\n",
    "model_load.cuda()\n",
    "model_load.eval()\n",
    "y = t[2]\n",
    "res_arg = model_load(t[0].cuda(), t[1].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例探查总结\n",
    "\n",
    "一些误判如下：\n",
    "- 一些容易修正的误判（加规则或者加CRF）\n",
    "'s, y: O, predict: I-ORG\n",
    "\n",
    "- 地点误判为机构\n",
    "United, y: B-LOC, predict: I-ORG Arab, y: I-LOC, predict: I-LOC Emirates, y: I-LOC, predict: I-ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Australia, y: B-LOC, predict: B-LOC\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Scorers, y: O, predict: O\n",
      ":, y: O, predict: O\n",
      "Shkvyrin, y: B-PER, predict: B-PER\n",
      "Igor, y: I-PER, predict: I-PER\n",
      "78, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "Shatskikh, y: B-PER, predict: B-PER\n",
      "Oleg, y: I-PER, predict: I-PER\n",
      "90, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Everton, y: B-ORG, predict: B-ORG\n",
      "16, y: O, predict: O\n",
      "6, y: O, predict: O\n",
      "6, y: O, predict: O\n",
      "4, y: O, predict: O\n",
      "25, y: O, predict: O\n",
      "20, y: O, predict: O\n",
      "24, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "In, y: O, predict: O\n",
      "another, y: O, predict: O\n",
      "attack, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "also, y: O, predict: O\n",
      "on, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "province, y: O, predict: O\n",
      "'s, y: O, predict: O\n",
      "south, y: O, predict: O\n",
      "coast, y: O, predict: O\n",
      "on, y: O, predict: O\n",
      "Thursday, y: O, predict: O\n",
      "night, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "two, y: O, predict: O\n",
      "men, y: O, predict: O\n",
      "were, y: O, predict: O\n",
      "shot, y: O, predict: O\n",
      "dead, y: O, predict: O\n",
      "near, y: O, predict: O\n",
      "Umkomaas, y: B-LOC, predict: B-ORG\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "CLEVELAND, y: B-ORG, predict: B-ORG\n",
      "AT, y: O, predict: O\n",
      "DETROIT, y: B-LOC, predict: B-LOC\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "\", y: O, predict: O\n",
      "If, y: O, predict: O\n",
      "they, y: O, predict: O\n",
      "ca, y: O, predict: O\n",
      "n't, y: O, predict: O\n",
      "move, y: O, predict: O\n",
      "because, y: O, predict: O\n",
      "they, y: O, predict: O\n",
      "are, y: O, predict: O\n",
      "too, y: O, predict: O\n",
      "weak, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "then, y: O, predict: O\n",
      "we, y: O, predict: O\n",
      "will, y: O, predict: O\n",
      "probably, y: O, predict: O\n",
      "consider, y: O, predict: O\n",
      "very, y: O, predict: O\n",
      "seriously, y: O, predict: O\n",
      "using, y: O, predict: O\n",
      "air, y: O, predict: O\n",
      "delivery, y: O, predict: O\n",
      "means, y: O, predict: O\n",
      "(, y: O, predict: O\n",
      "airdrops, y: O, predict: O\n",
      "), y: O, predict: O\n",
      "...It, y: O, predict: O\n",
      "'s, y: O, predict: O\n",
      "complex, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "it, y: O, predict: O\n",
      "'s, y: O, predict: O\n",
      "dangerous, y: O, predict: O\n",
      "for, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "air, y: O, predict: O\n",
      "crew, y: O, predict: O\n",
      "that, y: O, predict: O\n",
      "fly, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "there, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "it, y: O, predict: O\n",
      "will, y: O, predict: O\n",
      "have, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "be, y: O, predict: O\n",
      "absolutely, y: O, predict: O\n",
      "necessary, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "It, y: O, predict: O\n",
      "has, y: O, predict: O\n",
      "produced, y: O, predict: O\n",
      "1.5, y: O, predict: O\n",
      "million, y: O, predict: O\n",
      "hectolitres, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "7., y: O, predict: O\n",
      "Madlen, y: B-PER, predict: B-PER\n",
      "Brigger-Summermatter, y: I-PER, predict: I-PER\n",
      "(, y: O, predict: O\n",
      "Switzerland, y: B-LOC, predict: B-LOC\n",
      "), y: O, predict: O\n",
      "1:18.23, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Robert, y: B-PER, predict: B-PER\n",
      "Galvin, y: I-PER, predict: I-PER\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Attendance, y: O, predict: O\n",
      "33,000, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Kuwait, y: B-LOC, predict: B-LOC\n",
      "-, y: O, predict: O\n",
      "Jassem, y: B-PER, predict: B-PER\n",
      "Al-Huwaidi, y: I-PER, predict: I-PER\n",
      "9, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "44, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "More, y: O, predict: O\n",
      "than, y: O, predict: O\n",
      "21,000, y: O, predict: O\n",
      "people, y: O, predict: O\n",
      "have, y: O, predict: O\n",
      "been, y: O, predict: O\n",
      "killed, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "12-year-old, y: O, predict: O\n",
      "conflict, y: O, predict: O\n",
      "between, y: O, predict: O\n",
      "Turkish, y: B-MISC, predict: B-MISC\n",
      "security, y: O, predict: O\n",
      "forces, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "PKK, y: B-ORG, predict: B-ORG\n",
      ",, y: O, predict: O\n",
      "fighting, y: O, predict: O\n",
      "for, y: O, predict: O\n",
      "Kurdish, y: B-MISC, predict: B-MISC\n",
      "autonomy, y: O, predict: O\n",
      "or, y: O, predict: O\n",
      "independence, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "But, y: B-MISC, predict: O\n",
      "2, y: O, predict: O\n",
      "27/11/96, y: O, predict: B-ORG\n",
      "5,000, y: O, predict: O\n",
      "Burma, y: B-LOC, predict: B-LOC\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "17., y: O, predict: O\n",
      "Bibiana, y: B-PER, predict: B-PER\n",
      "Perez, y: I-PER, predict: I-PER\n",
      "(, y: O, predict: O\n",
      "Italy, y: B-LOC, predict: B-LOC\n",
      "), y: O, predict: O\n",
      "30, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "A, y: O, predict: O\n",
      "Santa, y: B-PER, predict: B-PER\n",
      "Claus, y: I-PER, predict: I-PER\n",
      "distributing, y: O, predict: O\n",
      "presents, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "workers, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "German, y: B-MISC, predict: B-MISC\n",
      "bank, y: O, predict: O\n",
      "on, y: O, predict: O\n",
      "Friday, y: O, predict: O\n",
      "nearly, y: O, predict: O\n",
      "ended, y: O, predict: O\n",
      "up, y: O, predict: O\n",
      "behind, y: O, predict: O\n",
      "bars, y: O, predict: O\n",
      "when, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "passing, y: O, predict: O\n",
      "police, y: O, predict: O\n",
      "patrol, y: O, predict: O\n",
      "thought, y: O, predict: O\n",
      "he, y: O, predict: O\n",
      "was, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "robber, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "disguise, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Torquay, y: B-ORG, predict: B-ORG\n",
      "22, y: O, predict: O\n",
      "8, y: O, predict: O\n",
      "4, y: O, predict: O\n",
      "10, y: O, predict: O\n",
      "22, y: O, predict: O\n",
      "24, y: O, predict: O\n",
      "28, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "8=, y: O, predict: O\n",
      "Carole, y: B-PER, predict: B-PER\n",
      "Montillet, y: I-PER, predict: I-PER\n",
      "(, y: O, predict: O\n",
      "France, y: B-LOC, predict: B-LOC\n",
      "), y: O, predict: O\n",
      "146, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "\", y: O, predict: O\n",
      "At, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "moment, y: O, predict: O\n",
      "there, y: O, predict: O\n",
      "is, y: O, predict: O\n",
      "no, y: O, predict: O\n",
      "evidence, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "two, y: O, predict: O\n",
      "cases, y: O, predict: O\n",
      "are, y: O, predict: O\n",
      "linked, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "League, y: O, predict: O\n",
      "duties, y: O, predict: O\n",
      "restricted, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "Barbarians, y: B-ORG, predict: B-LOC\n",
      "', y: O, predict: O\n",
      "selectorial, y: O, predict: O\n",
      "options, y: O, predict: O\n",
      "but, y: O, predict: O\n",
      "they, y: O, predict: O\n",
      "still, y: O, predict: O\n",
      "boast, y: O, predict: O\n",
      "13, y: O, predict: O\n",
      "internationals, y: O, predict: O\n",
      "including, y: O, predict: O\n",
      "England, y: B-LOC, predict: B-LOC\n",
      "full-back, y: O, predict: O\n",
      "Tim, y: B-PER, predict: B-PER\n",
      "Stimpson, y: I-PER, predict: I-PER\n",
      "and, y: O, predict: O\n",
      "recalled, y: O, predict: O\n",
      "wing, y: O, predict: O\n",
      "Tony, y: B-PER, predict: B-PER\n",
      "Underwood, y: I-PER, predict: I-PER\n",
      ",, y: O, predict: O\n",
      "plus, y: O, predict: O\n",
      "All, y: B-ORG, predict: B-ORG\n",
      "Black, y: I-ORG, predict: I-ORG\n",
      "forwards, y: O, predict: O\n",
      "Ian, y: B-PER, predict: B-PER\n",
      "Jones, y: I-PER, predict: I-PER\n",
      "and, y: O, predict: O\n",
      "Norm, y: B-PER, predict: B-PER\n",
      "Hewitt, y: I-PER, predict: I-PER\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "is, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "clear, y: O, predict: O\n",
      "signal, y: O, predict: O\n",
      "that, y: O, predict: O\n",
      "one, y: O, predict: O\n",
      "key, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "lines, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "foreign, y: O, predict: O\n",
      "policy, y: O, predict: O\n",
      "will, y: O, predict: O\n",
      "be, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "strengthening, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "trans-Atlantic, y: B-MISC, predict: O\n",
      "cooperation, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "creation, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "strategic, y: O, predict: O\n",
      "partnership, y: O, predict: O\n",
      "between, y: O, predict: O\n",
      "Europe, y: B-LOC, predict: B-LOC\n",
      "and, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "US, y: B-LOC, predict: B-LOC\n",
      ",, y: O, predict: O\n",
      "\", y: O, predict: O\n",
      "Foreign, y: O, predict: O\n",
      "Minister, y: O, predict: O\n",
      "Josef, y: B-PER, predict: B-PER\n",
      "Zieleniec, y: I-PER, predict: I-PER\n",
      "told, y: O, predict: O\n",
      "Reuters, y: B-ORG, predict: B-ORG\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "K.J., y: B-PER, predict: B-PER\n",
      "Matthew, y: I-PER, predict: I-PER\n",
      "said, y: O, predict: O\n",
      "at, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "Asia, y: B-MISC, predict: B-ORG\n",
      "Rubber, y: I-MISC, predict: I-ORG\n",
      "Markets, y: I-MISC, predict: I-ORG\n",
      "meeting, y: I-MISC, predict: O\n",
      "here, y: O, predict: O\n",
      "Indian, y: B-MISC, predict: B-MISC\n",
      "production, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "natural, y: O, predict: O\n",
      "rubber, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "1996/97, y: O, predict: O\n",
      "will, y: O, predict: O\n",
      "reach, y: O, predict: O\n",
      "547,000, y: O, predict: O\n",
      "tonnes, y: O, predict: O\n",
      "against, y: O, predict: O\n",
      "projected, y: O, predict: O\n",
      "demand, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "578,000, y: O, predict: O\n",
      "tonnes, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "gap, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "31,000, y: O, predict: O\n",
      "tonnes, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Cricket, y: O, predict: O\n",
      "-, y: O, predict: O\n",
      "Pakistan, y: B-LOC, predict: B-LOC\n",
      "beat, y: O, predict: O\n",
      "New, y: B-LOC, predict: B-LOC\n",
      "Zealand, y: I-LOC, predict: I-LOC\n",
      "by, y: O, predict: O\n",
      "46, y: O, predict: O\n",
      "runs, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "AMARILLO, y: B-LOC, predict: B-ORG\n",
      "1996-12-06, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "But, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "students, y: O, predict: O\n",
      "stressed, y: O, predict: O\n",
      "their, y: O, predict: O\n",
      "protests, y: O, predict: O\n",
      "were, y: O, predict: O\n",
      "non-political, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "they, y: O, predict: O\n",
      "had, y: O, predict: O\n",
      "no, y: O, predict: O\n",
      "contact, y: O, predict: O\n",
      "with, y: O, predict: O\n",
      "Suu, y: B-PER, predict: B-PER\n",
      "Kyi, y: I-PER, predict: I-PER\n",
      "'s, y: O, predict: O\n",
      "National, y: B-ORG, predict: B-MISC\n",
      "League, y: I-ORG, predict: I-MISC\n",
      "for, y: I-ORG, predict: O\n",
      "Democracy, y: I-ORG, predict: B-ORG\n",
      "(, y: O, predict: O\n",
      "NLD, y: B-ORG, predict: O\n",
      "), y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Total, y: O, predict: O\n",
      "730.0, y: O, predict: O\n",
      "9176.1, y: O, predict: O\n",
      "8188.2, y: O, predict: O\n",
      "164.2, y: O, predict: O\n",
      "3156.0, y: O, predict: O\n",
      "2686.7, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Guilin, y: B-LOC, predict: B-LOC\n",
      "is, y: O, predict: O\n",
      "well, y: O, predict: O\n",
      "known, y: O, predict: O\n",
      "for, y: O, predict: O\n",
      "its, y: O, predict: O\n",
      "mountain, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "river, y: O, predict: O\n",
      "scenery, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "is, y: O, predict: O\n",
      "one, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "China, y: B-LOC, predict: B-LOC\n",
      "'s, y: O, predict: O\n",
      "most, y: O, predict: O\n",
      "popular, y: O, predict: O\n",
      "tourist, y: O, predict: O\n",
      "destinations, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Ipswich, y: B-ORG, predict: B-ORG\n",
      "22, y: O, predict: O\n",
      "6, y: O, predict: O\n",
      "8, y: O, predict: O\n",
      "8, y: O, predict: O\n",
      "27, y: O, predict: O\n",
      "32, y: O, predict: O\n",
      "26, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "210, y: O, predict: O\n",
      "Brett, y: B-PER, predict: B-PER\n",
      "Liddle, y: I-PER, predict: I-PER\n",
      "75, y: O, predict: O\n",
      "65, y: O, predict: O\n",
      "70, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Today, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "Rottweilers, y: B-MISC, predict: B-ORG\n",
      "are, y: O, predict: O\n",
      "on, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "way, y: O, predict: O\n",
      "up, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "\", y: O, predict: O\n",
      "Rieck, y: B-PER, predict: B-PER\n",
      "said, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "A, y: O, predict: O\n",
      "five-year-old, y: O, predict: O\n",
      "girl, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "east, y: O, predict: O\n",
      "China, y: B-LOC, predict: B-LOC\n",
      "city, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "Tianjin, y: B-LOC, predict: B-ORG\n",
      "choked, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "almost, y: O, predict: O\n",
      "died, y: O, predict: O\n",
      "from, y: O, predict: O\n",
      "cigarette, y: O, predict: O\n",
      "smoke, y: O, predict: O\n",
      "at, y: O, predict: O\n",
      "her, y: O, predict: O\n",
      "grandfather, y: O, predict: O\n",
      "'s, y: O, predict: O\n",
      "birthday, y: O, predict: O\n",
      "with, y: O, predict: O\n",
      "relatives, y: O, predict: O\n",
      "smoking, y: O, predict: O\n",
      "for, y: O, predict: O\n",
      "hours, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "small, y: O, predict: O\n",
      "room, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "Wen, y: B-ORG, predict: B-ORG\n",
      "Hui, y: I-ORG, predict: I-ORG\n",
      "Bao, y: I-ORG, predict: I-ORG\n",
      "newspaper, y: O, predict: O\n",
      "said, y: O, predict: O\n",
      "on, y: O, predict: O\n",
      "Friday, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "Bath, y: B-ORG, predict: B-ORG\n",
      "35, y: O, predict: O\n",
      "Harlequins, y: B-ORG, predict: B-ORG\n",
      "20, y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n",
      "==================================================\n",
      "<START>, y: <START>, predict: <START>\n",
      "White, y: B-LOC, predict: B-ORG\n",
      "House, y: I-LOC, predict: I-ORG\n",
      "spokesman, y: O, predict: O\n",
      "Mike, y: B-PER, predict: B-PER\n",
      "McCurry, y: I-PER, predict: I-PER\n",
      "said, y: O, predict: O\n",
      "Clinton, y: B-PER, predict: B-PER\n",
      "\", y: O, predict: O\n",
      "plans, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "have, y: O, predict: O\n",
      "regular, y: O, predict: O\n",
      "news, y: O, predict: O\n",
      "conferences, y: O, predict: O\n",
      "\", y: O, predict: O\n",
      "during, y: O, predict: O\n",
      "his, y: O, predict: O\n",
      "second, y: O, predict: O\n",
      "term, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "<STOP>, y: <STOP>, predict: <STOP>\n"
     ]
    }
   ],
   "source": [
    "# true value\n",
    "y_ner_list = []\n",
    "for sent in y:\n",
    "    tmp = []\n",
    "    for token in sent:\n",
    "        tmp.append(ner_id2tag[token.item()])\n",
    "    y_ner_list.append(tmp)\n",
    "\n",
    "# predict\n",
    "res_ner_list = []\n",
    "for sent in res_arg:\n",
    "    tmp = []\n",
    "    for token in sent:\n",
    "        tmp.append(ner_id2tag[token.item()])\n",
    "    res_ner_list.append(tmp)\n",
    "\n",
    "for i, sent in enumerate(t[4]):\n",
    "    print('='*50)\n",
    "    for j, token in enumerate(sent):\n",
    "        print('{}, y: {}, predict: {}'.format(token, y_ner_list[i][j], res_ner_list[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
