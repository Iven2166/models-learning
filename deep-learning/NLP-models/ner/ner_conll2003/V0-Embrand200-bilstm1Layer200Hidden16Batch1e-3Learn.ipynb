{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "# np/pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# transformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# CRF\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = {\n",
    "    'num_tags':9,\n",
    "    'embedding_dim':200,\n",
    "    'vocab_size':23623,\n",
    "    'hidden_dim':100,\n",
    "    'batch_size':32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探查conll2003数据\n",
    "\n",
    "定义dataset，dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b09833690f540d3b602d629a12fff95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_udpos = torchtext.datasets.UDPOS(root='./torchtext_datasets_udpos/', split=('train','valid','test'))\n",
    "\n",
    "dataset_conll2003 = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['SOCCER',\n",
       "  '-',\n",
       "  'JAPAN',\n",
       "  'GET',\n",
       "  'LUCKY',\n",
       "  'WIN',\n",
       "  ',',\n",
       "  'CHINA',\n",
       "  'IN',\n",
       "  'SURPRISE',\n",
       "  'DEFEAT',\n",
       "  '.'],\n",
       " 'pos_tags': [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7],\n",
       " 'chunk_tags': [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0],\n",
       " 'ner_tags': [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_conll2003['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag2id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "\n",
    "ner_id2tag = {}\n",
    "for key in ner_tag2id.keys():\n",
    "    ner_id2tag[ner_tag2id[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
       " ['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ner_id_to_tags(ner_id_seq):\n",
    "    res1, res2 = [], []\n",
    "    for ner_id in ner_id_seq:\n",
    "        res1.append(ner_id2tag.get(ner_id, ''))\n",
    "        res2.append(ner_id2tag.get(ner_id, '-').split('-')[-1])\n",
    "    return res1, res2\n",
    "\n",
    "ner_id_to_tags(dataset_conll2003['train'][0]['ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length = 124\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "for k in dataset_conll2003.keys():\n",
    "    for i in dataset_conll2003[k]['tokens']:\n",
    "        m = max(m, len(i))\n",
    "print('max length = {}'.format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30289\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for k in dataset_conll2003.keys():\n",
    "    for tokens in dataset_conll2003[k]['tokens']:\n",
    "        for token in tokens:\n",
    "            if token not in word_to_ix:\n",
    "                word_to_ix[token] = len(word_to_ix)\n",
    "                \n",
    "Config['vocab_size'] = len(word_to_ix)\n",
    "\n",
    "print(Config['vocab_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_word2ix(word_to_ix, token_list):\n",
    "    res = list()\n",
    "    for token in token_list:\n",
    "        res.append(word_to_ix.get(token, len(word_to_ix)+1))\n",
    "    return {'token_ids':res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-e993f22e7c8f1977.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-9da2d1c0e9528511.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-586edb6c12fcdba7.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_conll2003_train=dataset_conll2003['train'].map(lambda x: func_word2ix(word_to_ix, x['tokens']))\n",
    "dataset_conll2003_train.set_format(type=\"torch\", columns=['token_ids','ner_tags'])\n",
    "\n",
    "dataset_conll2003_test=dataset_conll2003['test'].map(lambda x: func_word2ix(word_to_ix, x['tokens']))\n",
    "dataset_conll2003_test.set_format(type=\"torch\", columns=['token_ids','ner_tags'])\n",
    "\n",
    "dataset_conll2003_val=dataset_conll2003['validation'].map(lambda x: func_word2ix(word_to_ix, x['tokens']))\n",
    "dataset_conll2003_val.set_format(type=\"torch\", columns=['token_ids','ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.token_ids = self.data['token_ids'] # 在这变成torch.tensor，但长度不同\n",
    "        self.ner_tags = self.data['ner_tags']\n",
    "        self.tokens = self.data['tokens']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         curr = dict()\n",
    "#         curr['token_ids'] = self.token_ids\n",
    "#         curr['ner_tags'] = self.ner_tags\n",
    "        return self.token_ids[index], self.ner_tags[index], self.tokens[index]\n",
    "\n",
    "\n",
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    Padds batch of variable length\n",
    "\n",
    "    note: it converts things ToTensor manually here since the ToTensor transform\n",
    "    assume it takes in images rather than arbitrary tensors.\n",
    "    '''\n",
    "    x, y, z = zip(*batch)\n",
    "    x_lens = [len(x_i) for x_i in x]\n",
    "    y_lens = [len(y_i) for y_i in y]\n",
    "    x_pad = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y_pad = torch.nn.utils.rnn.pad_sequence(y, batch_first=True)\n",
    "    return x_pad, torch.tensor(x_lens), y_pad, torch.tensor(y_lens), z\n",
    "    \n",
    "dataset_conll2003_train_loader = DataLoader(\n",
    "    MyDataset(dataset_conll2003_train),\n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    collate_fn=lambda x: collate_fn_padd(x))\n",
    "\n",
    "\n",
    "dataset_conll2003_test_loader = DataLoader(\n",
    "    MyDataset(dataset_conll2003_test),\n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    collate_fn=lambda x: collate_fn_padd(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Katarina',\n",
       " 'Studenikova',\n",
       " '(',\n",
       " 'Slovakia',\n",
       " ')',\n",
       " 'beat',\n",
       " '6',\n",
       " '-',\n",
       " 'Karina',\n",
       " 'Habsudova']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset_conll2003_train_loader))[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041 3453\n"
     ]
    }
   ],
   "source": [
    "print(dataset_conll2003_train_loader.dataset.__len__(), dataset_conll2003_test_loader.dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # BiLSTM-model 给 emission 层定义参数\n",
    "        self.embedding_dim = self.config.get('embedding_dim', 200)\n",
    "        self.hidden_dim = self.config.get('hidden_dim', 200)\n",
    "        self.vocab_size = self.config.get('vocab_size', 30289)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.target_size = self.config.get('num_tags', 9)\n",
    "        self.num_layers = 1\n",
    "        self.batch_size = self.config.get('batch_size',16)\n",
    "        self.bidirectional = True\n",
    "\n",
    "        # lstm\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim//2,\n",
    "                            num_layers=self.num_layers, bidirectional=self.bidirectional)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, self.target_size)\n",
    "#         self.hidden_init = self.init_hidden()\n",
    "\n",
    "        # CRF-model\n",
    "        self.crf = CRF(self.config.get('num_tags', 9), batch_first=True)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers*2 if self.bidirectional else self.num_layers,\n",
    "                             self.batch_size, self.hidden_dim//2)\n",
    "        cell = torch.zeros(self.num_layers*2 if self.bidirectional else self.num_layers,\n",
    "                             self.batch_size, self.hidden_dim//2)\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, sent, sent_len):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sent: 输入的已转换为token_id的句子，(batch_len * sent_len * token_emb_len)\n",
    "        :param sent_len: tensor(list(int))\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        embeds = self.word_embeds(sent)\n",
    "        embed_packed = pack_padded_sequence(embeds, lengths=sent_len.to('cpu'),\n",
    "                                            batch_first=True,\n",
    "                                            enforce_sorted=False)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embed_packed) #, self.hidden_init)\n",
    "        lstm_out, lens = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        tag_score = self.hidden2tag(lstm_out)\n",
    "#         tag_score = nn.functional.softmax(tag_score, dim=-1)\n",
    "        return tag_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'V0-Embrand200-bilstm1Layer200Hidden16Batch1e-3Learn'\n",
    "model_lstm = BiLSTM_CRF(config=Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = model_lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(30289, 200)\n",
       "  (lstm): LSTM(200, 50, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=100, out_features=9, bias=True)\n",
       "  (crf): CRF(num_tags=9)\n",
       ")>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 3.5792683998664065), (0, 0.1334168085220698), (7, 6.580731691551936), (1, 3.4279629629629627), (2, 4.996589124460149), (5, 3.1687052598817305), (4, 6.108141348692104), (8, 19.58835978835979), (6, 19.554499183712664)]\n"
     ]
    }
   ],
   "source": [
    "# 手动计算验证权重\n",
    "import collections\n",
    "ner_tags_all = torch.cat(dataset_conll2003_train['ner_tags'])\n",
    "t=collections.Counter(ner_tags_all.numpy())\n",
    "res = []\n",
    "for k in t:\n",
    "    res.append((k, len(ner_tags_all)/9/t[k]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1334,  3.4280,  4.9966,  3.5793,  6.1081,  3.1687, 19.5545,  6.5807,\n",
       "        19.5884])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights=sklearn.utils.class_weight.compute_class_weight(\n",
    "    class_weight='balanced',classes=np.unique(ner_tags_all),y=ner_tags_all.numpy())\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=1e-1)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean', weight=class_weights.cuda()) \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_single_epoch(model, data_iter=None, optimizer=None, loss_fn=None, is_train=False):\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()   \n",
    "    correct_curr, correct_sum, loss_sum, loss_curr = 0, 0, 0, 0\n",
    "    loss_list, accuracy_list = [], []\n",
    "    print('Total (training) batch = {}'.format(len(data_iter)))\n",
    "    batch_i = 0\n",
    "    data_iter_len = data_iter.dataset.__len__() # total sample num.\n",
    "    batch_num = len(data_iter)\n",
    "    \n",
    "    batch_loss_list = list()\n",
    "    logits_list = list()\n",
    "    y_list, y_len_list = [], []\n",
    "    for batch_data in data_iter:\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_i += 1\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "#             model.zero_grad()\n",
    "        x, x_len, y, y_len = batch_data\n",
    "        x = x.cuda()\n",
    "        x_len = x_len.cuda()\n",
    "        y = y.cuda()\n",
    "        # model predict \n",
    "        logits = model(x, x_len)\n",
    "        assert logits.shape[0:2]==x.shape[0:2]\n",
    "        # compute loss \n",
    "        batch_loss = 0\n",
    "        for i in range(logits.size(0)): # num of samples in one batch\n",
    "            loss = loss_fn(logits[i], y[i])\n",
    "            batch_loss += loss \n",
    "        \n",
    "        batch_loss /= logits.size(0)\n",
    "        if is_train:\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 记录\n",
    "        batch_loss_list.append(batch_loss.item())\n",
    "        logits_list.append(logits)\n",
    "        y_list.append(y)\n",
    "        y_len_list.append(y_len)\n",
    "#         if batch_i%100==0:\n",
    "#             print(batch_loss.item())\n",
    "        \n",
    "        x = x.cpu()\n",
    "        x_len = x_len.cpu()\n",
    "        y = y.cpu()\n",
    "        \n",
    "#     print(sum(batch_loss_list)/batch_num)\n",
    "    return batch_loss_list, logits_list, y_list, y_len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_cal_accu_recall(logits_list=None, y_list=None, y_len_list=None):\n",
    "    eval_dict = {'tp':0,'tn':0,'fp':0,'fn':0,'others':0,'n_total':0}\n",
    "    n_total = 0\n",
    "    for logit_i, logit in enumerate(logits_list):\n",
    "        assert logit.shape[0:2] == y_list[logit_i].shape\n",
    "        batch_len = len(y_len_list[logit_i])\n",
    "        batch_max_seqlen = max(y_len_list[logit_i])\n",
    "        y_curr = y_list[logit_i]\n",
    "        logit_argmax = torch.argmax(logit, dim=2)\n",
    "\n",
    "        # get mask matrix\n",
    "        mask = torch.zeros((batch_len, batch_max_seqlen))\n",
    "        for mask_i in range(mask.shape[0]):\n",
    "            mask[mask_i][0:y_len_list[logit_i][mask_i]] = 1\n",
    "        assert sum(mask.sum(axis=1)==y_len_list[logit_i])//batch_len==1\n",
    "\n",
    "        # cal the tp,tn,fp,fn in this batch\n",
    "        N = mask.sum()\n",
    "        TP = ((logit_argmax>0)*(y_curr>0)*(logit_argmax==y_curr)*mask.cuda()).sum()\n",
    "        TN = ((logit_argmax==0)*(y_curr==0)*(logit_argmax==y_curr)*mask.cuda()).sum()\n",
    "        FP = ((logit_argmax>0)*(y_curr==0)*(logit_argmax!=y_curr)*mask.cuda()).sum()\n",
    "        FN = ((logit_argmax==0)*(y_curr>0)*(logit_argmax!=y_curr)*mask.cuda()).sum()\n",
    "        others = ((logit_argmax>0)*(y_curr>0)*(logit_argmax!=y_curr)*mask.cuda()).sum()\n",
    "\n",
    "        eval_dict['tp'] += TP.item()\n",
    "        eval_dict['tn'] += TN.item()\n",
    "        eval_dict['fp'] += FP.item()\n",
    "        eval_dict['fn'] += FN.item()\n",
    "        eval_dict['others'] += others.item()\n",
    "        eval_dict['n_total'] += N.item()\n",
    "\n",
    "        accu = (TP+TN) / N\n",
    "        recall = TP / (TP + FN)\n",
    "#         print(accu, recall)\n",
    "    \n",
    "    if False:\n",
    "        print('Total accu = {:.2f}% recall = {:.2f}%'.format(\n",
    "            (eval_dict['tp'] + eval_dict['tn'])/eval_dict['n_total']*100, \n",
    "            eval_dict['tp']/(eval_dict['tp'] + eval_dict['fn'])*100))\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(model, data_iter=None, loss_fn=None):\n",
    "    model.eval()\n",
    "    correct_curr, correct_sum, loss_sum, loss_curr = 0, 0, 0, 0\n",
    "    loss_list, accuracy_list = [], []\n",
    "    print('Total (training) batch = {}'.format(len(data_iter)))\n",
    "    batch_i = 0\n",
    "    data_iter_len = data_iter.dataset.__len__() # total sample num.\n",
    "    batch_num = len(data_iter)\n",
    "    \n",
    "    batch_loss_list = list()\n",
    "    logits_list = list()\n",
    "    y_list, y_len_list = [], []\n",
    "    for batch_data in data_iter:\n",
    "        batch_i += 1\n",
    "        x, x_len, y, y_len = batch_data\n",
    "        x = x.cuda()\n",
    "        x_len = x_len.cuda()\n",
    "        y = y.cuda()\n",
    "        # model predict \n",
    "        logits = model(x, x_len)\n",
    "        assert logits.shape[0:2]==x.shape[0:2]\n",
    "        # compute loss \n",
    "        batch_loss = 0\n",
    "        for i in range(logits.size(0)): # num of samples in one batch\n",
    "            loss = loss_fn(logits[i], y[i])\n",
    "            batch_loss += loss \n",
    "        \n",
    "        batch_loss /= logits.size(0)\n",
    "        \n",
    "        # 记录\n",
    "        batch_loss_list.append(batch_loss.item())\n",
    "        logits_list.append(logits)\n",
    "        y_list.append(y)\n",
    "        y_len_list.append(y_len)\n",
    "#         if batch_i%100==0:\n",
    "#             print(batch_loss.item())\n",
    "        \n",
    "        x = x.cpu()\n",
    "        x_len = x_len.cpu()\n",
    "        y = y.cpu()\n",
    "        \n",
    "#     print(sum(batch_loss_list)/batch_num)\n",
    "    return batch_loss_list, logits_list, y_list, y_len_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Epoch = 0\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 1.11865732\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.18347435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  from ipykernel import kernelapp as app\n",
      "  2%|▏         | 1/50 [00:14<11:36, 14.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 68.88% recall = 73.88%\n",
      "Test :Total accu = 66.67% recall = 73.10%\n",
      "==================================================\n",
      "Epoch = 1\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.95371711\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.13154795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:28<11:33, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 73.31% recall = 81.14%\n",
      "Test :Total accu = 68.02% recall = 76.90%\n",
      "==================================================\n",
      "Epoch = 2\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.86974234\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.13118567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:43<11:13, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 75.42% recall = 84.35%\n",
      "Test :Total accu = 70.21% recall = 78.77%\n",
      "==================================================\n",
      "Epoch = 3\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.78000184\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.05750550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:57<10:55, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 78.71% recall = 87.11%\n",
      "Test :Total accu = 73.04% recall = 81.12%\n",
      "==================================================\n",
      "Epoch = 4\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.69043518\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.07245177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:11<10:41, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 81.42% recall = 89.64%\n",
      "Test :Total accu = 75.56% recall = 80.01%\n",
      "==================================================\n",
      "Epoch = 5\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.63456507\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.02980752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [01:25<10:28, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 83.23% recall = 91.27%\n",
      "Test :Total accu = 75.62% recall = 83.76%\n",
      "==================================================\n",
      "Epoch = 6\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.57226409\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.97377480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:40<10:14, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 84.95% recall = 92.35%\n",
      "Test :Total accu = 76.86% recall = 85.50%\n",
      "==================================================\n",
      "Epoch = 7\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.52990232\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.00981725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:54<10:00, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 85.97% recall = 93.43%\n",
      "Test :Total accu = 78.02% recall = 84.69%\n",
      "==================================================\n",
      "Epoch = 8\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.50117510\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.97807315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [02:08<09:47, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 86.64% recall = 93.91%\n",
      "Test :Total accu = 78.53% recall = 85.33%\n",
      "==================================================\n",
      "Epoch = 9\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.46029196\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.99282025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [02:23<09:35, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 87.79% recall = 94.57%\n",
      "Test :Total accu = 78.81% recall = 86.32%\n",
      "==================================================\n",
      "Epoch = 10\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.42851140\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.96449747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [02:37<09:21, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 88.65% recall = 95.18%\n",
      "Test :Total accu = 79.97% recall = 86.82%\n",
      "==================================================\n",
      "Epoch = 11\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.39430666\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.93988831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [02:51<09:03, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 89.75% recall = 95.71%\n",
      "Test :Total accu = 80.65% recall = 88.27%\n",
      "==================================================\n",
      "Epoch = 12\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.36089305\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.94421653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [03:05<08:44, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 90.65% recall = 96.35%\n",
      "Test :Total accu = 80.71% recall = 89.49%\n",
      "==================================================\n",
      "Epoch = 13\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.33112882\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.96129725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [03:19<08:31, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 91.41% recall = 96.85%\n",
      "Test :Total accu = 81.68% recall = 89.39%\n",
      "==================================================\n",
      "Epoch = 14\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.30692836\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.96005372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [03:34<08:19, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 92.00% recall = 97.07%\n",
      "Test :Total accu = 80.93% recall = 90.28%\n",
      "==================================================\n",
      "Epoch = 15\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.28493097\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.94898864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [03:48<08:06, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 92.47% recall = 97.43%\n",
      "Test :Total accu = 82.12% recall = 90.44%\n",
      "==================================================\n",
      "Epoch = 16\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.26307265\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.96223267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [04:03<07:54, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 93.00% recall = 97.69%\n",
      "Test :Total accu = 82.69% recall = 89.96%\n",
      "==================================================\n",
      "Epoch = 17\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.24401608\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.97229819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [04:17<07:40, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 93.49% recall = 98.01%\n",
      "Test :Total accu = 82.26% recall = 90.49%\n",
      "==================================================\n",
      "Epoch = 18\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.22807203\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.97947279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [04:32<07:29, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 93.96% recall = 98.15%\n",
      "Test :Total accu = 82.79% recall = 89.94%\n",
      "==================================================\n",
      "Epoch = 19\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.21176266\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.98192497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [04:47<07:15, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 94.31% recall = 98.30%\n",
      "Test :Total accu = 82.67% recall = 90.77%\n",
      "==================================================\n",
      "Epoch = 20\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.19795583\n",
      "Total (training) batch = 108\n",
      "testing loss = 0.99596207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [05:01<06:59, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 94.68% recall = 98.46%\n",
      "Test :Total accu = 82.89% recall = 90.96%\n",
      "==================================================\n",
      "Epoch = 21\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.18655092\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.03285700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [05:15<06:45, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 94.92% recall = 98.48%\n",
      "Test :Total accu = 83.26% recall = 90.43%\n",
      "==================================================\n",
      "Epoch = 22\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.17737473\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.01728134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [05:30<06:30, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 95.22% recall = 98.65%\n",
      "Test :Total accu = 82.95% recall = 90.79%\n",
      "==================================================\n",
      "Epoch = 23\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.17035055\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.03948058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [05:44<06:16, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 95.41% recall = 98.78%\n",
      "Test :Total accu = 83.20% recall = 90.35%\n",
      "==================================================\n",
      "Epoch = 24\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.16412671\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.05153486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [05:59<06:01, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 95.66% recall = 98.83%\n",
      "Test :Total accu = 83.17% recall = 90.80%\n",
      "==================================================\n",
      "Epoch = 25\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.15962023\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.05268786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [06:13<05:45, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 95.70% recall = 98.89%\n",
      "Test :Total accu = 83.26% recall = 90.58%\n",
      "==================================================\n",
      "Epoch = 26\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.15487291\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.06320943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [06:27<05:31, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 95.86% recall = 98.95%\n",
      "Test :Total accu = 83.67% recall = 90.35%\n",
      "==================================================\n",
      "Epoch = 27\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.15208751\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.06092695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [06:42<05:15, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 95.94% recall = 98.93%\n",
      "Test :Total accu = 83.52% recall = 90.78%\n",
      "==================================================\n",
      "Epoch = 28\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14894454\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.07232034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [06:56<05:01, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 95.99% recall = 99.01%\n",
      "Test :Total accu = 83.66% recall = 90.36%\n",
      "==================================================\n",
      "Epoch = 29\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14702654\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.07421950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [07:10<04:46, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.08% recall = 98.99%\n",
      "Test :Total accu = 83.59% recall = 90.45%\n",
      "==================================================\n",
      "Epoch = 30\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14549607\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.07082562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [07:24<04:31, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.10% recall = 99.01%\n",
      "Test :Total accu = 83.65% recall = 90.50%\n",
      "==================================================\n",
      "Epoch = 31\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14425158\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.07491624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [07:39<04:16, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.16% recall = 99.06%\n",
      "Test :Total accu = 83.72% recall = 90.24%\n",
      "==================================================\n",
      "Epoch = 32\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14316513\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.07943118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [07:53<04:01, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.20% recall = 99.02%\n",
      "Test :Total accu = 83.64% recall = 90.43%\n",
      "==================================================\n",
      "Epoch = 33\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14217226\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.07911226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [08:07<03:48, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.18% recall = 99.05%\n",
      "Test :Total accu = 83.65% recall = 90.35%\n",
      "==================================================\n",
      "Epoch = 34\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14122511\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.07895740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [08:22<03:36, 14.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.24% recall = 99.01%\n",
      "Test :Total accu = 83.57% recall = 90.51%\n",
      "==================================================\n",
      "Epoch = 35\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14091379\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.09017754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [08:36<03:22, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.18% recall = 99.06%\n",
      "Test :Total accu = 83.58% recall = 90.66%\n",
      "==================================================\n",
      "Epoch = 36\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.14014432\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08698147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [08:51<03:07, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.22% recall = 99.08%\n",
      "Test :Total accu = 83.76% recall = 90.42%\n",
      "==================================================\n",
      "Epoch = 37\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13936851\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08578591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [09:05<02:53, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.28% recall = 99.07%\n",
      "Test :Total accu = 83.77% recall = 90.41%\n",
      "==================================================\n",
      "Epoch = 38\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13897065\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.09148828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [09:19<02:38, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.28% recall = 99.06%\n",
      "Test :Total accu = 83.68% recall = 90.47%\n",
      "==================================================\n",
      "Epoch = 39\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13838445\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08890588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [09:34<02:23, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.27% recall = 99.07%\n",
      "Test :Total accu = 83.71% recall = 90.55%\n",
      "==================================================\n",
      "Epoch = 40\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13786344\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08369221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [09:48<02:09, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.28% recall = 99.06%\n",
      "Test :Total accu = 83.70% recall = 90.46%\n",
      "==================================================\n",
      "Epoch = 41\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13741606\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.09134561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [10:03<01:55, 14.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.30% recall = 99.07%\n",
      "Test :Total accu = 83.76% recall = 90.55%\n",
      "==================================================\n",
      "Epoch = 42\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13719902\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08847481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [10:17<01:40, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.28% recall = 99.09%\n",
      "Test :Total accu = 83.77% recall = 90.54%\n",
      "==================================================\n",
      "Epoch = 43\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13674707\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08606759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [10:32<01:26, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.30% recall = 99.09%\n",
      "Test :Total accu = 83.78% recall = 90.50%\n",
      "==================================================\n",
      "Epoch = 44\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13626052\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08623647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [10:46<01:12, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.33% recall = 99.09%\n",
      "Test :Total accu = 83.81% recall = 90.50%\n",
      "==================================================\n",
      "Epoch = 45\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13587303\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08735964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [11:01<00:57, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.34% recall = 99.08%\n",
      "Test :Total accu = 83.83% recall = 90.49%\n",
      "==================================================\n",
      "Epoch = 46\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13579427\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.09117054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [11:15<00:43, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.36% recall = 99.09%\n",
      "Test :Total accu = 83.81% recall = 90.57%\n",
      "==================================================\n",
      "Epoch = 47\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13563762\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.08889939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [11:29<00:28, 14.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.36% recall = 99.09%\n",
      "Test :Total accu = 83.81% recall = 90.51%\n",
      "==================================================\n",
      "Epoch = 48\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13522672\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.09315020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [11:44<00:14, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.37% recall = 99.08%\n",
      "Test :Total accu = 83.80% recall = 90.47%\n",
      "==================================================\n",
      "Epoch = 49\n",
      "==================================================\n",
      "Total (training) batch = 439\n",
      "training loss = 0.13509511\n",
      "Total (training) batch = 108\n",
      "testing loss = 1.09197418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:58<00:00, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Total accu = 96.36% recall = 99.08%\n",
      "Test :Total accu = 83.81% recall = 90.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "F1_test_max, F1_test_curr = 0, 0\n",
    "\n",
    "for epoch in tqdm(range(50)):\n",
    "    print('='*50)\n",
    "    print('Epoch = {}'.format(epoch))\n",
    "    print('='*50)\n",
    "    batch_loss_list, logits_list, y_list, y_len_list = train_eval_single_epoch(model_lstm, \n",
    "                                                                               dataset_conll2003_train_loader, \n",
    "                                                                               optimizer=optimizer,\n",
    "                                                                               loss_fn=loss_fn,\n",
    "                                                                               is_train=True)\n",
    "    scheduler.step() # 加上后好一些\n",
    "    print('training loss = {:.8f}'.format(sum(batch_loss_list)/len(batch_loss_list)))\n",
    "    test_batch_loss_list, test_logits_list, test_y_list, test_y_len_list = func_eval(model_lstm, \n",
    "                                                                               dataset_conll2003_test_loader, \n",
    "                                                                               loss_fn=loss_fn)\n",
    "    print('testing loss = {:.8f}'.format(sum(test_batch_loss_list)/len(test_batch_loss_list)))\n",
    "    \n",
    "    # 评估\n",
    "    train_dict = func_cal_accu_recall(logits_list=logits_list, y_list=y_list, y_len_list=y_len_list)\n",
    "    test_dict = func_cal_accu_recall(logits_list=test_logits_list, y_list=test_y_list, y_len_list=test_y_len_list)\n",
    "    \n",
    "    print('Train :Total accu = {:.2f}% recall = {:.2f}%'.format(\n",
    "            (train_dict['tp'] + train_dict['tn'])/train_dict['n_total']*100, \n",
    "            train_dict['tp']/(train_dict['tp'] + train_dict['fn'])*100))\n",
    "    \n",
    "    print('Test :Total accu = {:.2f}% recall = {:.2f}%'.format(\n",
    "            (test_dict['tp'] + test_dict['tn'])/test_dict['n_total']*100, \n",
    "            test_dict['tp']/(test_dict['tp'] + test_dict['fn'])*100))\n",
    "    \n",
    "    # save model if current f1 rate is better than previous ones \n",
    "    accu_test = (test_dict['tp'] + test_dict['tn'])/test_dict['n_total']\n",
    "    recall_test = test_dict['tp']/(test_dict['tp'] + test_dict['fn'])\n",
    "    F1_test_curr = 1/(1/accu_test+1/recall_test)\n",
    "    if F1_test_curr>F1_test_max:\n",
    "        torch.save(model_lstm.state_dict(), './models/model_v0_'+model_name+'epoch='+str(epoch)+\n",
    "                   'accu='+str(round(accu_test,4))+\n",
    "                   'recall='+str(round(recall_test,4))+\n",
    "                   'F1='+str(round(F1_test_curr,4)))\n",
    "    F1_test_max = max(F1_test_curr, F1_test_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练总结\n",
    "\n",
    "能发现验证集在 21 epoch 时已达到最好状态，之后train开始过拟合，eval 也没办法继续提升准确率。\n",
    "\n",
    "epoch=0\n",
    "- Train :Total accu = 68.88% recall = 73.88%\n",
    "- Test :Total accu = 66.67% recall = 73.10%\n",
    "\n",
    "epoch=10\n",
    "- Train :Total accu = 88.65% recall = 95.18%\n",
    "- Test :Total accu = 79.97% recall = 86.82%\n",
    "\n",
    "epoch=21\n",
    "- Train :Total accu = 94.92% recall = 98.48%\n",
    "- Test :Total accu = 83.26% recall = 90.43%\n",
    "\n",
    "epoch=50\n",
    "- Train :Total accu = 96.36% recall = 99.08%\n",
    "- Test :Total accu = 83.81% recall = 90.54%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型加载及结果探查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_load = BiLSTM_CRF(config=Config)\n",
    "model_lstm_load.load_state_dict(torch.load('./models/model_v0_V0-Embrand200-bilstm1Layer200Hidden16Batch1e-3Learnepoch=27accu=0.835210509314095recall=0.9078498293515358F1=0.43500830208429403'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(30289, 200)\n",
       "  (lstm): LSTM(200, 50, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=100, out_features=9, bias=True)\n",
       "  (crf): CRF(num_tags=9)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   81,    16,    88,  ...,     0,     0,     0],\n",
      "        [ 1835,  2823,  3733,  ...,     0,     0,     0],\n",
      "        [ 1921, 23318,     0,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [10569,  1592,  2087,  ...,     0,     0,     0],\n",
      "        [30177, 30178, 27834,  ...,     0,     0,     0],\n",
      "        [27575,  2169,   693,  ...,     0,     0,     0]]), tensor([33, 25,  2, 10, 13,  4,  7,  4,  8, 14, 16, 24, 11,  8, 22,  2, 33, 26,\n",
      "         7,  8,  2,  1, 19,  6, 36,  2,  3, 20,  6,  8,  3,  8]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [5, 6, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [3, 0, 0,  ..., 0, 0, 0],\n",
      "        [5, 6, 0,  ..., 0, 0, 0],\n",
      "        [3, 0, 0,  ..., 0, 0, 0]]), tensor([33, 25,  2, 10, 13,  4,  7,  4,  8, 14, 16, 24, 11,  8, 22,  2, 33, 26,\n",
      "         7,  8,  2,  1, 19,  6, 36,  2,  3, 20,  6,  8,  3,  8]), (['He', 'said', 'that', 'the', 'procedure', 'to', 'insert', 'a', 'device', 'into', 'Havel', \"'s\", 'throat', ',', 'done', 'after', 'his', 'breathing', 'worsened', 'on', 'Thursday', ',', 'had', 'helped', ',', 'and', 'the', 'president', \"'s\", 'condition', 'significantly', 'improved', '.'], ['South', 'Korea', 'made', 'virtually', 'certain', 'of', 'an', 'Asian', 'Cup', 'quarter-final', 'spot', 'with', 'a', '4-2', 'win', 'over', 'Indonesia', 'in', 'a', 'Group', 'A', 'match', 'on', 'Saturday', '.'], ['Attendance', '33,000'], ['United', 'Arab', 'Emirates', '3', 'Kuwait', '2', '(', 'halftime', '0-2', ')'], ['Mohammed', 'Rashid', '\"', 'is', 'a', 'terrorist', 'who', 'deserves', 'to', 'be', 'behind', 'bars', '.'], ['Enfield', '1', 'Peterborough', '1'], ['Anke', 'Baler', '(', 'Germany', ')', '41.76', '.'], ['Results', 'of', 'National', 'Basketball'], ['Wimbledon', '16', '9', '4', '3', '29', '17', '31'], ['Seventy-seven', 'students', 'were', 'found', 'with', 'the', 'watches', 'and', 'disqualified', ',', 'O', 'Globo', 'said', '.'], ['\"', 'The', 'market', \"'s\", 'very', 'healthy', ',', 'we', \"'re\", 'buying', ',', '\"', 'said', 'another', 'trader', '.'], ['He', 'was', 'jeered', 'and', 'whistled', 'at', 'by', 'a', 'small', 'group', 'of', 'League', 'supporters', 'when', 'he', 'arrived', 'for', 'a', 'visit', 'marked', 'by', 'heavy', 'security', '.'], ['SQUASH', '-', 'EYLES', 'WITHIN', 'SIGHT', 'OF', 'FIFTH', 'TITLE', 'OF', 'YEAR', '.'], ['NY', 'ISLANDERS', '7', '11', '8', '65', '72', '22'], ['Cold', 'front', 'bringing', 'light', 'rains', 'to', 'the', 'coast', 'of', 'Tamaulipas', ',', 'but', 'with', 'the', 'rest', 'of', 'the', 'Gulf', 'in', 'clear', 'skies', '.'], ['NORTHEAST', 'DIVISION'], ['A', '39-yard', 'interference', 'penalty', 'against', 'Philadelphia', \"'s\", 'Troy', 'Vincent', 'set', 'up', 'Faulk', \"'s\", 'first', 'score', 'around', 'left', 'end', 'that', 'capped', 'an', '80-yard', 'march', '5:17', 'into', 'the', 'game', 'and', 'the', 'rout', 'was', 'on', '.'], ['\"', 'FIFA', 'has', 'named', 'the', 'Liberian', 'for', 'its', '1996', 'Fair', 'Play', 'award', 'and', 'it', 'is', 'not', 'going', 'to', 'change', 'its', 'decision', ',', '\"', 'Havelange', 'said', '.'], ['14.', 'Hilde', 'Gerg', '(', 'Germany', ')', '1:49.84'], ['Hereford', '22', '6', '5', '11', '23', '31', '23'], ['West', 'Indies'], ['40-1'], ['Violent', 'crime', 'has', 'soared', 'since', 'the', 'collapse', 'of', 'communism', 'in', '1989', 'as', 'Bulgaria', 'moves', 'to', 'a', 'market', 'economy', '.'], ['FREESTYLE', 'SKIING-WORLD', 'CUP', 'AERIALS', 'RESULTS', '.'], ['The', 'ODS', ',', 'a', 'party', 'in', 'which', 'Klaus', 'often', 'tries', 'to', 'emulate', 'the', 'style', 'of', 'former', 'British', 'Prime', 'Minister', 'Margaret', 'Thatcher', ',', 'has', 'been', 'in', 'control', 'of', 'Czech', 'politics', 'since', 'winning', 'general', 'elections', 'in', '1992', '.'], ['GLASGOW', '1996-12-07'], ['BUFFALO', 'AT', 'HARTFORD'], ['The', 'issue', 'is', 'of', 'moral', 'and', 'legal', 'nature', ',', 'because', 'its', 'financial', 'significance', 'is', 'small', ',', '\"', 'Rosati', 'said', '.'], ['Her', 'name', 'was', 'not', 'released', '.'], ['Scarborough', '21', '7', '9', '5', '30', '27', '30'], ['ABU', 'DHABI', '1996-12-07'], ['Hercules', '15', '2', '2', '11', '11', '29', '8']))\n"
     ]
    }
   ],
   "source": [
    "t=next(iter(dataset_conll2003_test_loader))\n",
    "print(t)\n",
    "model_lstm_load.cuda()\n",
    "model_lstm_load.eval()\n",
    "y = t[2]\n",
    "res = model_lstm_load(t[0].cuda(), t[1].cuda())\n",
    "res_arg = torch.argmax(res, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例探查总结\n",
    "\n",
    "一些误判如下：\n",
    "- 一些容易修正的误判（加规则或者加CRF）\n",
    "'s, y: O, predict: I-ORG\n",
    "\n",
    "- 地点误判为机构\n",
    "United, y: B-LOC, predict: I-ORG Arab, y: I-LOC, predict: I-LOC Emirates, y: I-LOC, predict: I-ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "He, y: O, predict: O\n",
      "said, y: O, predict: O\n",
      "that, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "procedure, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "insert, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "device, y: O, predict: O\n",
      "into, y: O, predict: O\n",
      "Havel, y: B-PER, predict: B-PER\n",
      "'s, y: O, predict: I-ORG\n",
      "throat, y: O, predict: B-MISC\n",
      ",, y: O, predict: O\n",
      "done, y: O, predict: O\n",
      "after, y: O, predict: O\n",
      "his, y: O, predict: O\n",
      "breathing, y: O, predict: B-PER\n",
      "worsened, y: O, predict: B-PER\n",
      "on, y: O, predict: O\n",
      "Thursday, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "had, y: O, predict: O\n",
      "helped, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "president, y: O, predict: O\n",
      "'s, y: O, predict: I-LOC\n",
      "condition, y: O, predict: O\n",
      "significantly, y: O, predict: O\n",
      "improved, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "South, y: B-LOC, predict: B-MISC\n",
      "Korea, y: I-LOC, predict: I-LOC\n",
      "made, y: O, predict: O\n",
      "virtually, y: O, predict: O\n",
      "certain, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "an, y: O, predict: O\n",
      "Asian, y: B-MISC, predict: B-MISC\n",
      "Cup, y: I-MISC, predict: I-MISC\n",
      "quarter-final, y: O, predict: O\n",
      "spot, y: O, predict: O\n",
      "with, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "4-2, y: O, predict: O\n",
      "win, y: O, predict: O\n",
      "over, y: O, predict: O\n",
      "Indonesia, y: B-LOC, predict: B-LOC\n",
      "in, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "Group, y: O, predict: O\n",
      "A, y: O, predict: O\n",
      "match, y: O, predict: O\n",
      "on, y: O, predict: O\n",
      "Saturday, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "Attendance, y: O, predict: O\n",
      "33,000, y: O, predict: O\n",
      "==================================================\n",
      "United, y: B-LOC, predict: I-ORG\n",
      "Arab, y: I-LOC, predict: I-LOC\n",
      "Emirates, y: I-LOC, predict: I-ORG\n",
      "3, y: O, predict: O\n",
      "Kuwait, y: B-LOC, predict: B-LOC\n",
      "2, y: O, predict: O\n",
      "(, y: O, predict: O\n",
      "halftime, y: O, predict: O\n",
      "0-2, y: O, predict: O\n",
      "), y: O, predict: O\n",
      "==================================================\n",
      "Mohammed, y: B-PER, predict: B-PER\n",
      "Rashid, y: I-PER, predict: I-PER\n",
      "\", y: O, predict: O\n",
      "is, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "terrorist, y: O, predict: O\n",
      "who, y: O, predict: O\n",
      "deserves, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "be, y: O, predict: O\n",
      "behind, y: O, predict: O\n",
      "bars, y: O, predict: I-MISC\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "Enfield, y: B-ORG, predict: O\n",
      "1, y: O, predict: O\n",
      "Peterborough, y: B-ORG, predict: B-ORG\n",
      "1, y: O, predict: O\n",
      "==================================================\n",
      "Anke, y: B-PER, predict: B-PER\n",
      "Baler, y: I-PER, predict: I-PER\n",
      "(, y: O, predict: O\n",
      "Germany, y: B-LOC, predict: B-LOC\n",
      "), y: O, predict: I-ORG\n",
      "41.76, y: O, predict: I-MISC\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "Results, y: O, predict: O\n",
      "of, y: O, predict: I-ORG\n",
      "National, y: B-ORG, predict: B-ORG\n",
      "Basketball, y: I-ORG, predict: I-ORG\n",
      "==================================================\n",
      "Wimbledon, y: B-ORG, predict: B-ORG\n",
      "16, y: O, predict: O\n",
      "9, y: O, predict: O\n",
      "4, y: O, predict: O\n",
      "3, y: O, predict: O\n",
      "29, y: O, predict: O\n",
      "17, y: O, predict: O\n",
      "31, y: O, predict: O\n",
      "==================================================\n",
      "Seventy-seven, y: O, predict: I-PER\n",
      "students, y: O, predict: O\n",
      "were, y: O, predict: O\n",
      "found, y: O, predict: O\n",
      "with, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "watches, y: O, predict: I-MISC\n",
      "and, y: O, predict: I-ORG\n",
      "disqualified, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "O, y: B-ORG, predict: B-ORG\n",
      "Globo, y: I-ORG, predict: I-ORG\n",
      "said, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "\", y: O, predict: O\n",
      "The, y: O, predict: O\n",
      "market, y: O, predict: O\n",
      "'s, y: O, predict: O\n",
      "very, y: O, predict: O\n",
      "healthy, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "we, y: O, predict: O\n",
      "'re, y: O, predict: O\n",
      "buying, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "\", y: O, predict: O\n",
      "said, y: O, predict: O\n",
      "another, y: O, predict: O\n",
      "trader, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "He, y: O, predict: O\n",
      "was, y: O, predict: O\n",
      "jeered, y: O, predict: B-ORG\n",
      "and, y: O, predict: I-ORG\n",
      "whistled, y: O, predict: I-PER\n",
      "at, y: O, predict: O\n",
      "by, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "small, y: O, predict: O\n",
      "group, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "League, y: B-LOC, predict: I-MISC\n",
      "supporters, y: O, predict: B-MISC\n",
      "when, y: O, predict: O\n",
      "he, y: O, predict: O\n",
      "arrived, y: O, predict: O\n",
      "for, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "visit, y: O, predict: O\n",
      "marked, y: O, predict: O\n",
      "by, y: O, predict: O\n",
      "heavy, y: O, predict: O\n",
      "security, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "SQUASH, y: O, predict: O\n",
      "-, y: O, predict: O\n",
      "EYLES, y: B-PER, predict: B-PER\n",
      "WITHIN, y: O, predict: I-PER\n",
      "SIGHT, y: O, predict: I-ORG\n",
      "OF, y: O, predict: B-LOC\n",
      "FIFTH, y: O, predict: I-PER\n",
      "TITLE, y: O, predict: O\n",
      "OF, y: O, predict: I-MISC\n",
      "YEAR, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "NY, y: B-ORG, predict: B-LOC\n",
      "ISLANDERS, y: I-ORG, predict: B-LOC\n",
      "7, y: O, predict: O\n",
      "11, y: O, predict: O\n",
      "8, y: O, predict: O\n",
      "65, y: O, predict: O\n",
      "72, y: O, predict: O\n",
      "22, y: O, predict: O\n",
      "==================================================\n",
      "Cold, y: O, predict: I-ORG\n",
      "front, y: O, predict: O\n",
      "bringing, y: O, predict: O\n",
      "light, y: O, predict: O\n",
      "rains, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "coast, y: O, predict: B-ORG\n",
      "of, y: O, predict: O\n",
      "Tamaulipas, y: B-LOC, predict: I-LOC\n",
      ",, y: O, predict: O\n",
      "but, y: O, predict: O\n",
      "with, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "rest, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "Gulf, y: B-LOC, predict: B-MISC\n",
      "in, y: O, predict: O\n",
      "clear, y: O, predict: O\n",
      "skies, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "NORTHEAST, y: O, predict: O\n",
      "DIVISION, y: O, predict: O\n",
      "==================================================\n",
      "A, y: O, predict: O\n",
      "39-yard, y: O, predict: I-PER\n",
      "interference, y: O, predict: I-PER\n",
      "penalty, y: O, predict: I-ORG\n",
      "against, y: O, predict: O\n",
      "Philadelphia, y: B-LOC, predict: B-LOC\n",
      "'s, y: O, predict: O\n",
      "Troy, y: B-PER, predict: B-PER\n",
      "Vincent, y: I-PER, predict: I-PER\n",
      "set, y: O, predict: O\n",
      "up, y: O, predict: O\n",
      "Faulk, y: B-PER, predict: B-PER\n",
      "'s, y: O, predict: O\n",
      "first, y: O, predict: O\n",
      "score, y: O, predict: O\n",
      "around, y: O, predict: O\n",
      "left, y: O, predict: O\n",
      "end, y: O, predict: O\n",
      "that, y: O, predict: O\n",
      "capped, y: O, predict: O\n",
      "an, y: O, predict: O\n",
      "80-yard, y: O, predict: O\n",
      "march, y: O, predict: O\n",
      "5:17, y: O, predict: O\n",
      "into, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "game, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "rout, y: O, predict: B-ORG\n",
      "was, y: O, predict: B-MISC\n",
      "on, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "\", y: O, predict: O\n",
      "FIFA, y: B-ORG, predict: B-ORG\n",
      "has, y: O, predict: O\n",
      "named, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "Liberian, y: B-MISC, predict: B-MISC\n",
      "for, y: O, predict: O\n",
      "its, y: O, predict: O\n",
      "1996, y: O, predict: O\n",
      "Fair, y: O, predict: O\n",
      "Play, y: O, predict: O\n",
      "award, y: O, predict: O\n",
      "and, y: O, predict: O\n",
      "it, y: O, predict: O\n",
      "is, y: O, predict: O\n",
      "not, y: O, predict: O\n",
      "going, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "change, y: O, predict: O\n",
      "its, y: O, predict: O\n",
      "decision, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "\", y: O, predict: O\n",
      "Havelange, y: B-PER, predict: B-ORG\n",
      "said, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "14., y: O, predict: O\n",
      "Hilde, y: B-PER, predict: I-ORG\n",
      "Gerg, y: I-PER, predict: I-PER\n",
      "(, y: O, predict: O\n",
      "Germany, y: B-LOC, predict: B-LOC\n",
      "), y: O, predict: B-LOC\n",
      "1:49.84, y: O, predict: I-ORG\n",
      "==================================================\n",
      "Hereford, y: B-ORG, predict: B-ORG\n",
      "22, y: O, predict: O\n",
      "6, y: O, predict: O\n",
      "5, y: O, predict: O\n",
      "11, y: O, predict: O\n",
      "23, y: O, predict: O\n",
      "31, y: O, predict: O\n",
      "23, y: O, predict: O\n",
      "==================================================\n",
      "West, y: B-LOC, predict: B-LOC\n",
      "Indies, y: I-LOC, predict: I-LOC\n",
      "==================================================\n",
      "40-1, y: O, predict: B-ORG\n",
      "==================================================\n",
      "Violent, y: O, predict: B-MISC\n",
      "crime, y: O, predict: O\n",
      "has, y: O, predict: O\n",
      "soared, y: O, predict: O\n",
      "since, y: O, predict: O\n",
      "the, y: O, predict: O\n",
      "collapse, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "communism, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "1989, y: O, predict: I-LOC\n",
      "as, y: O, predict: O\n",
      "Bulgaria, y: B-LOC, predict: B-LOC\n",
      "moves, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "market, y: O, predict: O\n",
      "economy, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "FREESTYLE, y: O, predict: O\n",
      "SKIING-WORLD, y: B-MISC, predict: I-PER\n",
      "CUP, y: I-MISC, predict: I-MISC\n",
      "AERIALS, y: O, predict: I-MISC\n",
      "RESULTS, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "The, y: O, predict: O\n",
      "ODS, y: B-ORG, predict: B-PER\n",
      ",, y: O, predict: O\n",
      "a, y: O, predict: O\n",
      "party, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "which, y: O, predict: O\n",
      "Klaus, y: B-PER, predict: B-PER\n",
      "often, y: O, predict: O\n",
      "tries, y: O, predict: O\n",
      "to, y: O, predict: O\n",
      "emulate, y: O, predict: B-PER\n",
      "the, y: O, predict: O\n",
      "style, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "former, y: O, predict: O\n",
      "British, y: B-MISC, predict: B-MISC\n",
      "Prime, y: O, predict: O\n",
      "Minister, y: O, predict: O\n",
      "Margaret, y: B-PER, predict: B-PER\n",
      "Thatcher, y: I-PER, predict: B-LOC\n",
      ",, y: O, predict: B-PER\n",
      "has, y: O, predict: O\n",
      "been, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "control, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "Czech, y: B-LOC, predict: B-MISC\n",
      "politics, y: O, predict: O\n",
      "since, y: O, predict: O\n",
      "winning, y: O, predict: O\n",
      "general, y: O, predict: O\n",
      "elections, y: O, predict: O\n",
      "in, y: O, predict: O\n",
      "1992, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "GLASGOW, y: B-LOC, predict: B-LOC\n",
      "1996-12-07, y: O, predict: O\n",
      "==================================================\n",
      "BUFFALO, y: B-ORG, predict: B-MISC\n",
      "AT, y: O, predict: O\n",
      "HARTFORD, y: B-LOC, predict: I-ORG\n",
      "==================================================\n",
      "The, y: O, predict: O\n",
      "issue, y: O, predict: O\n",
      "is, y: O, predict: O\n",
      "of, y: O, predict: O\n",
      "moral, y: O, predict: O\n",
      "and, y: O, predict: I-ORG\n",
      "legal, y: O, predict: O\n",
      "nature, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "because, y: O, predict: O\n",
      "its, y: O, predict: O\n",
      "financial, y: O, predict: O\n",
      "significance, y: O, predict: B-ORG\n",
      "is, y: O, predict: O\n",
      "small, y: O, predict: O\n",
      ",, y: O, predict: O\n",
      "\", y: O, predict: O\n",
      "Rosati, y: B-PER, predict: B-PER\n",
      "said, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "Her, y: O, predict: O\n",
      "name, y: O, predict: O\n",
      "was, y: O, predict: O\n",
      "not, y: O, predict: O\n",
      "released, y: O, predict: O\n",
      "., y: O, predict: O\n",
      "==================================================\n",
      "Scarborough, y: B-ORG, predict: B-ORG\n",
      "21, y: O, predict: O\n",
      "7, y: O, predict: O\n",
      "9, y: O, predict: O\n",
      "5, y: O, predict: O\n",
      "30, y: O, predict: O\n",
      "27, y: O, predict: O\n",
      "30, y: O, predict: O\n",
      "==================================================\n",
      "ABU, y: B-LOC, predict: I-LOC\n",
      "DHABI, y: I-LOC, predict: I-MISC\n",
      "1996-12-07, y: O, predict: O\n",
      "==================================================\n",
      "Hercules, y: B-ORG, predict: B-ORG\n",
      "15, y: O, predict: O\n",
      "2, y: O, predict: O\n",
      "2, y: O, predict: O\n",
      "11, y: O, predict: O\n",
      "11, y: O, predict: O\n",
      "29, y: O, predict: O\n",
      "8, y: O, predict: O\n"
     ]
    }
   ],
   "source": [
    "# true value\n",
    "y_ner_list = []\n",
    "for sent in y:\n",
    "    tmp = []\n",
    "    for token in sent:\n",
    "        tmp.append(ner_id2tag[token.item()])\n",
    "    y_ner_list.append(tmp)\n",
    "\n",
    "# predict\n",
    "res_ner_list = []\n",
    "for sent in res_arg:\n",
    "    tmp = []\n",
    "    for token in sent:\n",
    "        tmp.append(ner_id2tag[token.item()])\n",
    "    res_ner_list.append(tmp)\n",
    "\n",
    "for i, sent in enumerate(t[4]):\n",
    "    print('='*50)\n",
    "    for j, token in enumerate(sent):\n",
    "        print('{}, y: {}, predict: {}'.format(token, y_ner_list[i][j], res_ner_list[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
