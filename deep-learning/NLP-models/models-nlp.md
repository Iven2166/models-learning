
# æ·±åº¦å­¦ä¹  - NLPéƒ¨åˆ†

- æ ¸å¿ƒå‚è€ƒ
  - hugging-face 
    - [å®˜ç½‘](https://huggingface.co/)
    - [notebooks](https://huggingface.co/docs/transformers/v4.20.0/en/notebooks)
    - [course](https://huggingface.co/course/chapter2/2?fw=pt)
    - [transformer](https://huggingface.co/docs/transformers/model_doc/albert#transformers.AlbertForTokenClassification)
  - ä¸­æ–‡é¢„è®­ç»ƒ
    - [bert-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
  - pytorch 
    - [torchtextï¼Œå«æœ‰æ•°æ®é›†](https://github.com/pytorch/text)
  - [bertå¯è§†åŒ–](https://huggingface.co/exbert/?model=bert-large-uncased&modelKind=bidirectional&sentence=Do%20not%20meddle%20in%20the%20affairs%20of%20wizards,%20for%20they%20are%20subtle%20and%20quick%20to%20anger.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15&threshold=0.36&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)

- å­¦æœ¯è®ºæ–‡ / å­¦ä¼š
  - [ACL-Annual Meeting of the Association for Computational Linguistics `é€‰æ‹©ACLä¸ºè¯­è¨€æ¨¡å‹`](https://aclanthology.org/)
  - [EMNLP - 2020](https://2020.emnlp.org/papers/main)

- ä¸€äº›åšå®¢å‚è€ƒ
  
  - [rumor - å¦‚ä½•ç³»ç»Ÿæ€§åœ°å­¦ä¹ NLP è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ - `å…³æ³¨é‡Œé¢å¯¹ç»¼è¿°çš„å¼•ç”¨`](https://www.zhihu.com/question/27529154/answer/1643865710)
    - [NLPå¿«é€Ÿå…¥é—¨è·¯çº¿åŠä»»åŠ¡è¯¦è§£ - `å…¶ä»–ç»¼è¿°`](https://mp.weixin.qq.com/s/zrziuFLRvbG8axG48QpFvg)
    - [æ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»ï½œæ¨¡å‹&ä»£ç &æŠ€å·§](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247485854&idx=1&sn=040d51b0424bdee66f96d63d4ecfbe7e&chksm=9bb980faacce09ec069afa79c903b1e3a5c0d3679c41092e16b2fdd6949aa059883474d0c2af&token=1473678595&lang=zh_CN&scene=21#wechat_redirect)
  - [nlp-tutorial](https://github.com/graykode/nlp-tutorial)
  - [éå¸¸ç‰›é€¼çš„nlp_courseï¼ï¼ï¼](https://github.com/yandexdataschool/nlp_course)
  - [nlp-roadmap](https://github.com/graykode/nlp-roadmap)
  - [Transformers-å¤§é›†åˆ](https://github.com/huggingface/transformers)
  - [ä¸ªäººåšå®¢1](https://wmathor.com/index.php/archives/1399/)
  - [è‡ªç„¶è¯­è¨€å¤„ç†å…¥é—¨-ä¸€äº›åè¯å’Œæ¦‚å¿µ](http://www.fanyeong.com/2018/02/13/introduction_to_nlp/)
  - [Archive | Deep Learning for Natural Language Processing](https://machinelearningmastery.com/category/natural-language-processing/)
  - [NLP101-github](https://github.com/Huffon/NLP101)
  - [text-classification](https://github.com/zhengwsh/text-classification)
  - [meta-research](https://github.com/orgs/facebookresearch/repositories?q=&type=all&language=python&sort=)
  



## å‘å±•å†å²æ•´ä½“æ„ŸçŸ¥
ä»`ç»¼è¿°`ã€åšå®¢ä¸æ–­ç§¯ç´¯ï¼Œè‡ªå·±æ•´ç†å‡ºæ•´ä½“å‘å±•çš„å†å²ï¼Œå†é€ä¸ªæŒæ¡

![img.png](pics/img.png)

```markdown 
-- https://markmap.js.org/repl
# NLP
## ç»å…¸æ¨¡å‹

### ç»Ÿè®¡æœºå™¨å­¦ä¹ 
#### ......

### ç¥ç»ç½‘ç»œ
#### è¯å‘é‡
##### NNLM
##### Word2Vec
##### FaseText
##### Glove
#### CNN
#### RNN & LSTM & GRU
#### Bert 
#### ElMo?
```

ç»¼è¿°1:
[2020 A Survey on Text Classification: From Shallow to Deep Learning](https://arxiv.org/pdf/2008.00364v2.pdf)

![img_1.png](pics/img_from-shallow-to-deep-learning-fig2.png)

## TextCNN

é‡è¦å‚è€ƒ

| type 	| paper                                                                                                             	| intro                                                                         	| link                                  	|
|------	|-------------------------------------------------	|-------------------------------------------------	|----------------	|
| åŸä½œ 	| 2014-Convolutional Neural Networks for Sentence Classification                                                         	| (1) CNN-random-init <br> (2)CNN-static <br> (3)CNN-non-static <br> (4)CNN-multichannel 	| [link](https://aclanthology.org/D14-1181.pdf) 	|
| è¡ç”Ÿè§£è¯»1 	| 2016-A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification 	|                                                                                        	| [link](https://arxiv.org/pdf/1510.03820.pdf)  	|
| æˆ‘çš„è®°å½• |[TextCNN-readme.md](/TextCNN/TextCNN-readme.md)||
| æˆ‘çš„ä»£ç å¤ç° |jupyteræ–‡ä»¶|||


## RNN

**(1) æ¨¡å‹ç®€è¿°**

å‚è€ƒ: [d2l-8.4. å¾ªç¯ç¥ç»ç½‘ç»œ](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html)

æ¨¡å‹æ€»ä½“å…¬å¼ï¼š

å‡è®¾æˆ‘ä»¬åœ¨æ—¶é—´æ­¥ $t$ æœ‰å°æ‰¹é‡è¾“å…¥ 

$\mathbf{X}_ {t} \in \mathbb{R}^{n \times d}$,
$\mathbf{H}_ {t} \in \mathbb{R}^{n \times h}$,
$\mathbf{W}_ {hh} \in \mathbb{R}^{h \times h}$,
$\mathbf{W}_ {xh} \in \mathbb{R}^{d \times h}$,
$\mathbf{b}_ {h} \in \mathbb{R}^{1 \times h}$,
$\mathbf{b}_ {q} \in \mathbb{R}^{1 \times q}$,

å½“å‰æ—¶é—´æ­¥éšè—å˜é‡ç”±å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ä¸å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—å˜é‡ä¸€èµ·è®¡ç®—å¾—å‡ºï¼š

$$\mathbf{H}_ {t} = \phi ( \mathbf{X}_ {t} \mathbf{W}_ {xh} + \mathbf{H}_ {t-1} \mathbf{W}_ {hh}  + \mathbf{b}_ {h} )$$

å¯¹äºæ—¶é—´æ­¥$t$ï¼Œè¾“å‡ºå±‚çš„è¾“å‡ºç±»ä¼¼äºå¤šå±‚æ„ŸçŸ¥æœºä¸­çš„è®¡ç®—ï¼š

$$\mathbf{O}_ {t} = \mathbf{H}_ {t} \mathbf{W}_ {hq} + \mathbf{b}_ {q} $$


æŒ‡æ ‡ï¼šå›°æƒ‘åº¦(perplexity)ï¼š

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªåºåˆ—ä¸­æ‰€æœ‰çš„ $n$ ä¸ªè¯å…ƒçš„äº¤å‰ç†µæŸå¤±çš„å¹³å‡å€¼æ¥è¡¡é‡:

$$\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_ {t-1}, \ldots, x_1)$$

å…¶ä¸­$P$ç”±è¯­è¨€æ¨¡å‹ç»™å‡ºï¼Œ
$x_t$æ˜¯åœ¨æ—¶é—´æ­¥$t$ä»è¯¥åºåˆ—ä¸­è§‚å¯Ÿåˆ°çš„å®é™…è¯å…ƒã€‚
è¿™ä½¿å¾—ä¸åŒé•¿åº¦çš„æ–‡æ¡£çš„æ€§èƒ½å…·æœ‰äº†å¯æ¯”æ€§ã€‚

$$\exp\left(-\frac{1}{n} \sum_ {t=1}^n \log P(x_ t \mid x_ {t-1}, \ldots, x_ 1)\right).$$

* åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯å®Œç¾åœ°ä¼°è®¡æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º1ã€‚
  åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„å›°æƒ‘åº¦ä¸º1ã€‚
* åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯é¢„æµ‹æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º0ã€‚
  åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦æ˜¯æ­£æ— ç©·å¤§ã€‚
* åœ¨åŸºçº¿ä¸Šï¼Œè¯¥æ¨¡å‹çš„é¢„æµ‹æ˜¯è¯è¡¨çš„æ‰€æœ‰å¯ç”¨è¯å…ƒä¸Šçš„å‡åŒ€åˆ†å¸ƒã€‚
  åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦ç­‰äºè¯è¡¨ä¸­å”¯ä¸€è¯å…ƒçš„æ•°é‡ã€‚

å°ç»“

* å¾ªç¯ç¥ç»ç½‘ç»œçš„éšçŠ¶æ€å¯ä»¥æ•è·ç›´åˆ°å½“å‰æ—¶é—´æ­¥åºåˆ—çš„å†å²ä¿¡æ¯ã€‚
* å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‚æ•°æ•°é‡ä¸ä¼šéšç€æ—¶é—´æ­¥çš„å¢åŠ è€Œå¢åŠ ã€‚
* æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œåˆ›å»ºå­—ç¬¦çº§è¯­è¨€æ¨¡å‹ã€‚
* æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å›°æƒ‘åº¦æ¥è¯„ä»·è¯­è¨€æ¨¡å‹çš„è´¨é‡ã€‚


**(2) è®­ç»ƒ**

å‚è€ƒ
- [d2l-8.7. é€šè¿‡æ—¶é—´åå‘ä¼ æ’­](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html)
- [d2l-8.5. å¾ªç¯ç¥ç»ç½‘ç»œçš„ä»é›¶å¼€å§‹å®ç°](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)


å…¬å¼æœ‰ç©ºå†æ•´ç†

- å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨è®­ç»ƒä»¥å‰éœ€è¦åˆå§‹åŒ–çŠ¶æ€ï¼Œä¸è¿‡éšæœºæŠ½æ ·å’Œé¡ºåºåˆ’åˆ†ä½¿ç”¨åˆå§‹åŒ–æ–¹æ³•ä¸åŒã€‚[è§<éšæœºé‡‡æ ·å’Œé¡ºåºåˆ†åŒº>](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html)
- å½“ä½¿ç”¨é¡ºåºåˆ’åˆ†æ—¶ï¼Œæˆ‘ä»¬éœ€è¦åˆ†ç¦»æ¢¯åº¦ä»¥å‡å°‘è®¡ç®—é‡ã€‚
  > å…·ä½“æ¥è¯´ï¼Œå½“ä½¿ç”¨é¡ºåºåˆ†åŒºæ—¶ï¼Œ æˆ‘ä»¬åªåœ¨æ¯ä¸ªè¿­ä»£å‘¨æœŸçš„å¼€å§‹ä½ç½®åˆå§‹åŒ–éšçŠ¶æ€ã€‚ ç”±äºä¸‹ä¸€ä¸ªå°æ‰¹é‡æ•°æ®ä¸­çš„ç¬¬ ğ‘– ä¸ªå­åºåˆ—æ ·æœ¬ ä¸å½“å‰ç¬¬ ğ‘– ä¸ªå­åºåˆ—æ ·æœ¬ç›¸é‚»ï¼Œ å› æ­¤å½“å‰å°æ‰¹é‡æ•°æ®æœ€åä¸€ä¸ªæ ·æœ¬çš„éšçŠ¶æ€ï¼Œ å°†ç”¨äºåˆå§‹åŒ–ä¸‹ä¸€ä¸ªå°æ‰¹é‡æ•°æ®ç¬¬ä¸€ä¸ªæ ·æœ¬çš„éšçŠ¶æ€ã€‚ è¿™æ ·ï¼Œå­˜å‚¨åœ¨éšçŠ¶æ€ä¸­çš„åºåˆ—çš„å†å²ä¿¡æ¯ å¯ä»¥åœ¨ä¸€ä¸ªè¿­ä»£å‘¨æœŸå†…æµç»ç›¸é‚»çš„å­åºåˆ—ã€‚ ç„¶è€Œï¼Œåœ¨ä»»ä½•ä¸€ç‚¹éšçŠ¶æ€çš„è®¡ç®—ï¼Œ éƒ½ä¾èµ–äºåŒä¸€è¿­ä»£å‘¨æœŸä¸­å‰é¢æ‰€æœ‰çš„å°æ‰¹é‡æ•°æ®ï¼Œ è¿™ä½¿å¾—æ¢¯åº¦è®¡ç®—å˜å¾—å¤æ‚ã€‚ ä¸ºäº†é™ä½è®¡ç®—é‡ï¼Œåœ¨å¤„ç†ä»»ä½•ä¸€ä¸ªå°æ‰¹é‡æ•°æ®ä¹‹å‰ï¼Œ æˆ‘ä»¬å…ˆåˆ†ç¦»æ¢¯åº¦ï¼Œä½¿å¾—éšçŠ¶æ€çš„æ¢¯åº¦è®¡ç®—æ€»æ˜¯é™åˆ¶åœ¨ä¸€ä¸ªå°æ‰¹é‡æ•°æ®çš„æ—¶é—´æ­¥å†…ã€‚
- åœ¨è¿›è¡Œä»»ä½•é¢„æµ‹ä¹‹å‰ï¼Œæ¨¡å‹é€šè¿‡é¢„çƒ­æœŸè¿›è¡Œè‡ªæˆ‘æ›´æ–°ï¼ˆä¾‹å¦‚ï¼Œè·å¾—æ¯”åˆå§‹å€¼æ›´å¥½çš„éšçŠ¶æ€ï¼‰ã€‚
- æ¢¯åº¦è£å‰ªå¯ä»¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œä½†ä¸èƒ½åº”å¯¹æ¢¯åº¦æ¶ˆå¤±
- çŸ©é˜µ $\mathbf{W}_ {hh} \in \mathbb{R}^{h \times h}$ çš„é«˜æ¬¡å¹‚å¯èƒ½å¯¼è‡´ç¥ç»ç½‘ç»œç‰¹å¾å€¼çš„å‘æ•£æˆ–æ¶ˆå¤±ï¼Œå°†ä»¥æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±çš„å½¢å¼è¡¨ç°ã€‚
- æˆªæ–­æ˜¯è®¡ç®—æ–¹ä¾¿æ€§å’Œæ•°å€¼ç¨³å®šæ€§çš„éœ€è¦ã€‚æˆªæ–­åŒ…æ‹¬ï¼šè§„åˆ™æˆªæ–­å’Œ
  éšæœºæˆªæ–­(å®ç°æ–¹æ³•æ˜¯æœ‰ä¸€å®šæ¦‚ç‡åœ¨æŸä¸ªæ—¶é—´æ­¥ä¸Šæˆªæ–­ï¼Œå› æ­¤ä¸ç”¨å…¨éƒ¨è®¡ç®—ï¼›ä½†æ€»ä½“çš„æœŸæœ›æ˜¯1ï¼Œç›¸å½“äºç†è®ºå…¬å¼ï¼›æŸäº›æ—¶é—´æ­¥ä¼šæœ‰æ›´å¤§çš„æ¢¯åº¦æƒé‡)ã€‚
- ä¸ºäº†è®¡ç®—çš„æ•ˆç‡ï¼Œâ€œé€šè¿‡æ—¶é—´åå‘ä¼ æ’­â€åœ¨è®¡ç®—æœŸé—´ä¼šç¼“å­˜ä¸­é—´å€¼ã€‚


> æ¢¯åº¦çˆ†ç‚¸çš„å…¬å¼æ¨å¯¼: https://zhuanlan.zhihu.com/p/109519044
> ![img.png](./pics/rnn-gradient-decrease1.png)



**(3)æ³¨æ„ç‚¹**

QAï¼š

- ä¸ºä»€ä¹ˆåšå­—ç¬¦è€Œéé¢„æµ‹å•è¯ï¼Ÿ
  - å› ä¸ºå­—ç¬¦çš„vocab_sizeå°ï¼Œone-hotçš„å‘é‡ä¹Ÿå°ã€‚è€Œå•è¯åˆ™å¾ˆå¤§


---
- å‚è€ƒ
    - [æ·±åº¦å­¦ä¹ ä¹‹å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰](https://www.cnblogs.com/Luv-GEM/p/10703906.html)
    - [TensorFlowä¹‹RNNï¼šå †å RNNã€LSTMã€GRUåŠåŒå‘LSTM](https://www.cnblogs.com/Luv-GEM/p/10788849.html)
    - [å¾ªç¯ç¥ç»ç½‘ç»œä¹‹LSTMå’ŒGRU](https://www.cnblogs.com/Luv-GEM/p/10705967.html)
    - [rnnå„ç±»æ¨¡å‹å¤ç°](https://github.com/spro/practical-pytorch)
    - [pytorchå®˜æ–¹æ–‡ç« ](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#creating-the-network)
    - [rnn-å«æœ‰å¾ˆå¤šç°å®æ¡ˆä¾‹](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)


## é—¨æ§å¾ªç¯å•å…ƒï¼ˆgated recurrent unitsï¼ŒGRUï¼‰

å‚è€ƒ
- [9.1. é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰](https://zh.d2l.ai/chapter_recurrent-modern/gru.html)

RNNéœ€è¦è§£å†³çš„é—®é¢˜ï¼Œå¯ä»¥ç”± GRU å’Œ LSTMæ¥è§£å†³ï¼š
- é•¿æœŸè®°å¿† - è¿›è¡Œå­˜å‚¨ï¼šæ—©æœŸè§‚æµ‹å€¼å¯¹é¢„æµ‹æ‰€æœ‰æœªæ¥è§‚æµ‹å€¼å…·æœ‰éå¸¸é‡è¦çš„æ„ä¹‰ã€‚`æˆ‘ä»¬å¸Œæœ›æœ‰æŸäº›æœºåˆ¶èƒ½å¤Ÿåœ¨ä¸€ä¸ªè®°å¿†å…ƒé‡Œå­˜å‚¨é‡è¦çš„æ—©æœŸä¿¡æ¯`ã€‚ å¦‚æœæ²¡æœ‰è¿™æ ·çš„æœºåˆ¶ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ç»™è¿™ä¸ªè§‚æµ‹å€¼æŒ‡å®šä¸€ä¸ªéå¸¸å¤§çš„æ¢¯åº¦ï¼Œ å› ä¸ºå®ƒä¼šå½±å“æ‰€æœ‰åç»­çš„è§‚æµ‹å€¼ã€‚
- é€‰æ‹©é—å¿˜ - è¿›è¡Œè·³è¿‡ï¼šä¸€äº›è¯å…ƒæ²¡æœ‰ç›¸å…³çš„è§‚æµ‹å€¼ã€‚ ä¾‹å¦‚ï¼Œåœ¨å¯¹ç½‘é¡µå†…å®¹è¿›è¡Œæƒ…æ„Ÿåˆ†ææ—¶ï¼Œ å¯èƒ½æœ‰ä¸€äº›è¾…åŠ©HTMLä»£ç ä¸ç½‘é¡µä¼ è¾¾çš„æƒ…ç»ªæ— å…³ã€‚ `æˆ‘ä»¬å¸Œæœ›æœ‰ä¸€äº›æœºåˆ¶æ¥è·³è¿‡éšçŠ¶æ€è¡¨ç¤ºä¸­çš„æ­¤ç±»è¯å…ƒã€‚`
- é€»è¾‘ä¸­æ–­ - è¿›è¡Œé‡ç½®ï¼šåºåˆ—çš„å„ä¸ªéƒ¨åˆ†ä¹‹é—´å­˜åœ¨é€»è¾‘ä¸­æ–­ã€‚ ä¾‹å¦‚ï¼Œä¹¦çš„ç« èŠ‚ä¹‹é—´å¯èƒ½ä¼šæœ‰è¿‡æ¸¡å­˜åœ¨ã€‚æˆ‘ä»¬å¸Œæœ›é‡ç½®æˆ‘ä»¬çš„å†…éƒ¨çŠ¶æ€è¡¨ç¤º
- å…¶ä»–éœ€è§£å†³çš„ï¼šæ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸


![img.png](./pics/GRU.png)

- Reset Gateï¼šé‡ç½®é—¨ã€‚ ä¸ $H_ {t-1}$ è¿›è¡Œç‚¹ç§¯ï¼Œç”±äºæ˜¯ $(0,1)$ çš„èŒƒå›´ï¼Œå› æ­¤èƒ½å¤Ÿé€‰æ‹©é—å¿˜ $H_ {t-1}$ï¼Œä»è€Œå‡å°‘ä»¥å¾€çŠ¶æ€çš„å½±å“
- Update Gateï¼šæ›´æ–°é—¨ã€‚ å½“ $Z_ {t}$ åå‘ 1 æ—¶ï¼Œåˆ™é€‰æ‹©æ›´å¤šçš„ $H_ {t-1}$ï¼Œå³è®°ä½æ›´é•¿æœŸçš„ä¿¡æ¯ï¼›å¦‚æœåå‘äº0ï¼Œåˆ™é€‰æ‹©æ›´å¤šçš„å€™é€‰éšçŠ¶æ€ï¼Œå³å«æœ‰ $X_t$çš„å½“å‰ä¿¡æ¯

å°ç»“ï¼š
- é—¨æ§å¾ªç¯ç¥ç»ç½‘ç»œå¯ä»¥æ›´å¥½åœ°æ•è·æ—¶é—´æ­¥è·ç¦»å¾ˆé•¿çš„åºåˆ—ä¸Šçš„ä¾èµ–å…³ç³»ã€‚
- é‡ç½®é—¨æœ‰åŠ©äºæ•è·åºåˆ—ä¸­çš„çŸ­æœŸä¾èµ–å…³ç³»ã€‚
- æ›´æ–°é—¨æœ‰åŠ©äºæ•è·åºåˆ—ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚
- é‡ç½®é—¨æ‰“å¼€æ—¶ï¼Œé—¨æ§å¾ªç¯å•å…ƒåŒ…å«åŸºæœ¬å¾ªç¯ç¥ç»ç½‘ç»œï¼›æ›´æ–°é—¨æ‰“å¼€æ—¶ï¼Œé—¨æ§å¾ªç¯å•å…ƒå¯ä»¥è·³è¿‡å­åºåˆ—ã€‚


## é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆlong short-term memoryï¼ŒLSTMï¼‰ 
å‚è€ƒ
- [9.2. é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰](https://zh.d2l.ai/chapter_recurrent-modern/lstm.html)


![img.png](./pics/lstm-1.png)

å°ç»“
- é•¿çŸ­æœŸè®°å¿†ç½‘ç»œæœ‰ä¸‰ç§ç±»å‹çš„é—¨ï¼šè¾“å…¥é—¨ã€é—å¿˜é—¨å’Œè¾“å‡ºé—¨ã€‚
- é•¿çŸ­æœŸè®°å¿†ç½‘ç»œçš„éšè—å±‚è¾“å‡ºåŒ…æ‹¬â€œéšçŠ¶æ€â€å’Œâ€œè®°å¿†å…ƒâ€ã€‚åªæœ‰éšçŠ¶æ€ä¼šä¼ é€’åˆ°è¾“å‡ºå±‚ï¼Œè€Œè®°å¿†å…ƒå®Œå…¨å±äºå†…éƒ¨ä¿¡æ¯ã€‚
- é•¿çŸ­æœŸè®°å¿†ç½‘ç»œå¯ä»¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ã€‚

- å‚è€ƒ
  - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  - [ä¸ºä½•èƒ½è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Ÿ- Why LSTMs Stop Your Gradients From Vanishing](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html)
  - [ä¸ºä½•èƒ½è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Ÿ- ä¸­æ–‡ç¿»è¯‘](https://zhuanlan.zhihu.com/p/109519044)
  

## RNNç°‡çš„æ¯”è¾ƒç‚¹

- RNNæœ€é•¿åºåˆ—ä¸€èˆ¬ä¸ºå‡ åï¼Œéš¾ä»¥åšåˆ°ä¸Šç™¾çš„é•¿åº¦ï¼ˆæ‰€ä»¥éšæœºæŠ½æ ·å³å¯ï¼Œè¿˜å¯ä»¥é¿å¼€overfité—®é¢˜ï¼Œä¸ä¸€å®šè¦å› ä¸ºåºåˆ—çš„å…³è”æ€§è€Œç”¨é¡ºåºåˆ†åŒºï¼‰
- GRUçš„æ›´æ–°é—¨ $Z_ {t}$ æ˜¯åŠ æƒæ±‚å’Œï¼ˆ1-zä¸zï¼‰ï¼Œè€ŒLSTMçš„è®°å¿†å•å…ƒ $C_ {t}$ æ˜¯åŠ å’Œå¾—åˆ°çš„ï¼Œè€Œ $H_ {t}$ æ˜¯ç”±è¾“å‡ºé—¨ $O_ {t}$å†³å®šæ˜¯å¦é‡ç½®



## fastText 

- å®˜æ–¹å‚è€ƒ
  - å®˜æ–¹ï¼šhttps://fasttext.cc/
  - gitï¼šhttps://github.com/facebookresearch/fastText
  - å®˜æ–¹-æ–‡æœ¬åˆ†ç±»ï¼šhttps://fasttext.cc/docs/en/supervised-tutorial.html
  - FAQï¼šhttps://fasttext.cc/docs/en/faqs.html
    - reduce the size of my fastText models
- è®ºæ–‡
  - åŸä½œ(2016)ï¼šhttps://arxiv.org/pdf/1607.01759.pdf 
    - å®éªŒæ•°æ®ï¼š1  Sentiment analysis 2 tag predictionsï¼ˆæœ‰å¤šä¸ªtagçš„æƒ…å†µï¼‰
  
> A simple and efficient baseline for sentence classification is to represent sentences as bag of words (BoW) and train a linear classifier, e.g., a logistic regression or an SVM (Joachims, 1998; Fan et al., 2008). However, linear classifiers do not share parameters among features and classes. This possibly limits their generalization in the context of large output space where some classes have very few examples. Common solutions to this problem are to factorize the linear classifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or to use multilayer neural networks (Collobert and Weston, 2008; Zhang et al., 2015).
>
> å¥å­åˆ†ç±»çš„ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„åŸºçº¿æ˜¯å°†å¥å­è¡¨ç¤ºä¸ºè¯è¢‹ (BoW) å¹¶è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ï¼Œä¾‹å¦‚é€»è¾‘å›å½’æˆ– SVM (Joachims, 1998; Fan et al., 2008)ã€‚ ç„¶è€Œï¼Œçº¿æ€§åˆ†ç±»å™¨ä¸å…±äº«ç‰¹å¾å’Œç±»ä¹‹é—´çš„å‚æ•°ã€‚ è¿™å¯èƒ½ä¼šé™åˆ¶å®ƒä»¬åœ¨å¤§è¾“å‡ºç©ºé—´çš„ä¸Šä¸‹æ–‡ä¸­çš„æ³›åŒ–ï¼Œå…¶ä¸­æŸäº›ç±»çš„ç¤ºä¾‹å¾ˆå°‘ã€‚ è¿™ä¸ªé—®é¢˜çš„å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯å°†çº¿æ€§åˆ†ç±»å™¨åˆ†è§£ä¸ºä½ç§©çŸ©é˜µï¼ˆSchutzeï¼Œ1992ï¼›Mikolov ç­‰äººï¼Œ2013ï¼‰æˆ–ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œï¼ˆCollobert å’Œ Westonï¼Œ2008ï¼›Zhang ç­‰äººï¼Œ2015ï¼‰ã€‚




## Transformer

è®ºæ–‡è§£è¯»
- ç²¾è¯»è®ºæ–‡
  - [Transformer-<Attention-is-all-you-need>](https://github.com/Iven2166/models-learning/blob/main/paper-reading/Transformer-%3CAttention-is-all-you-need%3E.md)
  
- å…¶ä»–å‚è€ƒ
  - [å…¬ä¼—å·-å®ç°](https://mp.weixin.qq.com/s/cuYAt5B22CYtB3FBabjEIw)
  

**å†å²å‘å±•**

| è¿›ç¨‹     | è®ºæ–‡    | 
| :------------- | :------------- | 
| æ¦‚å¿µæå‡º      | åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸ,æ³¨æ„åŠ›è¿™ä¸€æ¦‚å¿µæœ€æ—©æ˜¯åœ¨è®¡ç®—æœºè§†è§‰ä¸­æå‡ºï¼Œç”¨æ¥æå–å›¾åƒç‰¹å¾ï¼[[Itti et al., 1998](https://www.cse.psu.edu/~rtc12/CSE597E/papers/Itti_etal98pami.pdf) ]æå‡ºäº†ä¸€ç§è‡ªä¸‹è€Œä¸Šçš„æ³¨æ„åŠ›æ¨¡å‹ï¼ è¯¥æ¨¡å‹é€šè¿‡æå–å±€éƒ¨çš„ä½çº§è§†è§‰ç‰¹å¾ï¼Œå¾—åˆ°ä¸€äº›æ½œåœ¨çš„æ˜¾è‘—ï¼ˆsalientï¼‰åŒºåŸŸï¼      | 
| å›¾åƒåˆ†ç±»      | åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œ [[Mnih et al., 2014](https://arxiv.org/pdf/1406.6247.pdf) ]åœ¨å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ä¸Šä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶æ¥è¿›è¡Œå›¾åƒåˆ†ç±»ï¼     | 
| æœºå™¨ç¿»è¯‘      | [[Bahdanau et al., 2014](https://arxiv.org/pdf/1409.0473.pdf) ]ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šå°†ç¿»è¯‘å’Œå¯¹é½åŒæ—¶è¿›è¡Œï¼     | 
| attention is all you need      | ç›®å‰ï¼Œ æ³¨æ„åŠ›æœºåˆ¶å·²ç»åœ¨è¯­éŸ³è¯†åˆ«ã€å›¾åƒæ ‡é¢˜ç”Ÿæˆã€é˜…è¯»ç†è§£ã€æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ ç¿»è¯‘ç­‰å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œ ä¹Ÿå˜å¾—è¶Šæ¥è¶Šæµè¡Œï¼ æ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ä¸ªé‡ è¦åº”ç”¨æ˜¯è‡ªæ³¨æ„åŠ›ï¼ è‡ªæ³¨æ„åŠ›å¯ä»¥ä½œä¸ºç¥ç»ç½‘ç»œä¸­çš„ä¸€å±‚æ¥ä½¿ç”¨ï¼Œ æœ‰æ•ˆåœ°å»ºæ¨¡é•¿ è·ç¦»ä¾èµ–é—®é¢˜ [[Attention is all you need, Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762.pdf) ]     | 



## Bert

- Bertè®ºæ–‡ï¼šhttps://arxiv.org/abs/1810.04805



## ElMo


- NNLM
  - è®ºæ–‡:  https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf 
  - è®ºæ–‡è§£è¯»: https://www.jianshu.com/p/be242ed3f314
  - [!!! NNLM çš„ PyTorch å®ç°](https://wmathor.com/index.php/archives/1442/)


## Word Embedding è¯åµŒå…¥
`word2vec, fastText, Glove, Bert`

æ¦‚å¿µä¸Šè€Œè¨€ï¼Œå®ƒæ˜¯æŒ‡æŠŠä¸€ä¸ªç»´æ•°ä¸ºæ‰€æœ‰è¯çš„æ•°é‡çš„é«˜ç»´ç©ºé—´åµŒå…¥åˆ°ä¸€ä¸ªç»´æ•°ä½å¾—å¤šçš„è¿ç»­å‘é‡ç©ºé—´ä¸­ï¼Œæ¯ä¸ªå•è¯æˆ–è¯ç»„è¢«æ˜ å°„ä¸ºå®æ•°åŸŸä¸Šçš„å‘é‡ã€‚
[wiki](https://zh.wikipedia.org/wiki/%E8%AF%8D%E5%B5%8C%E5%85%A5)

ç‰¹ç‚¹
- é«˜ç»´åº¦è½¬ä½ç»´åº¦è¡¨ç¤ºï¼šå•è¯ä¸ªæ•° |V| (one-hot) è½¬åŒ–ä¸ºæŒ‡å®šç»´åº¦
- è¯æ±‡ä¹‹é—´èƒ½å¤Ÿè¡¨ç¤ºç›¸ä¼¼åº¦
- å‘é‡çš„æ¯ä¸€ç»´æ²¡æœ‰å«ä¹‰

æ–‡æœ¬è¡¨ç¤ºæœ‰å“ªäº›æ–¹æ³•ï¼Ÿ
- åŸºäºone-hotã€tf-idfã€textrankç­‰çš„bag-of-words 
  - ç»´åº¦ç¾éš¾
  - è¯­ä¹‰é¸¿æ²Ÿ
- ä¸»é¢˜æ¨¡å‹ï¼šLSAï¼ˆSVDï¼‰ã€pLSAã€LDA
  - è®¡ç®—é‡å¤æ‚
- åŸºäºè¯å‘é‡çš„å›ºå®šè¡¨å¾ï¼šword2vecã€fastTextã€glove
  - ç›¸åŒä¸Šä¸‹æ–‡è¯­å¢ƒçš„è¯æœ‰ä¼¼å«ä¹‰
  - å›ºå®šè¡¨å¾æ— æ³•è¡¨ç¤º"ä¸€è¯å¤šä¹‰"ï¼ˆå› ä¸ºä¸€ä¸ªå•è¯åªæœ‰ä¸€ä¸ªembï¼Ÿï¼‰
- åŸºäºè¯å‘é‡çš„åŠ¨æ€è¡¨å¾ï¼šelmoã€GPTã€bert

å•ä¸ªä»‹ç»
- word2vec(2013)
  - è®ºæ–‡ï¼š
    - åŸä½œè€…-[word2vecæ€æƒ³](https://arxiv.org/pdf/1301.3781.pdf) (ä¹Ÿè®¨è®ºäº†å’ŒNNLMç­‰çš„åŒºåˆ«)
    - åŸä½œè€…-Skip-gramæ¨¡å‹çš„ä¸¤ä¸ªç­–ç•¥ï¼š[Hierarchical Softmax å’Œ Negative Sampling](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
    - åŸä½œè€…çš„[åšå£«è®ºæ–‡ï¼ˆ2012ï¼‰ï¼Œé€‚åˆç”¨äºäº†è§£å†å²](https://www.fit.vut.cz/study/phd-thesis-file/283/283.pdf)
    - å…¶ä»–ä½œè€…è§£è¯»
      - [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)
      - [word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)
  - è¦ç‚¹
    - Huffman Tree éœå¤«æ›¼äºŒå‰æ ‘ï¼šæƒå€¼æ›´é«˜çš„ç¦»æ ‘è¶Šè¿‘
  - åšå®¢å‚è€ƒï¼š
    - [word2vecåŸç†(ä¸€) CBOWä¸Skip-Gramæ¨¡å‹åŸºç¡€](https://www.cnblogs.com/pinard/p/7160330.html)
    - [word2vecåŸç†(äºŒ) åŸºäºHierarchical Softmaxçš„æ¨¡å‹](https://www.cnblogs.com/pinard/p/7243513.html)
    
  - å…¶å®word2vecå’ŒCo-Occurrence Vectorçš„æ€æƒ³æ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œéƒ½æ˜¯åŸºäºä¸€ä¸ªç»Ÿè®¡å­¦ä¸Šçš„å‡è®¾ï¼šç»å¸¸åœ¨åŒä¸€ä¸ªä¸Šä¸‹æ–‡å‡ºç°çš„å•è¯æ˜¯ç›¸ä¼¼çš„ã€‚åªæ˜¯ä»–ä»¬çš„å®ç°æ–¹å¼æ˜¯ä¸ä¸€æ ·çš„ï¼Œå‰è€…æ˜¯é‡‡ç”¨è¯é¢‘ç»Ÿè®¡ï¼Œé™ç»´ï¼ŒçŸ©é˜µåˆ†è§£ç­‰ç¡®å®šæ€§æŠ€æœ¯ï¼›è€Œåè€…åˆ™é‡‡ç”¨äº†ç¥ç»ç½‘ç»œè¿›è¡Œä¸ç¡®å®šé¢„æµ‹ï¼Œå®ƒçš„æå‡ºä¸»è¦æ˜¯é‡‡ç”¨ç¥ç»ç½‘ç»œä¹‹åè®¡ç®—å¤æ‚åº¦å’Œæœ€ç»ˆæ•ˆæœéƒ½æ¯”ä¹‹å‰çš„æ¨¡å‹è¦å¥½ã€‚æ‰€ä»¥é‚£ç¯‡æ–‡ç« çš„æ ‡é¢˜æ‰å«ï¼šEfficient Estimation of Word Representations in Vector Spaceã€‚[å‚è€ƒ](http://www.fanyeong.com/2017/10/10/word2vec/) 


- Glove
  - [åŸè®ºæ–‡](https://nlp.stanford.edu/pubs/glove.pdf)
  - [å®˜ç½‘](https://nlp.stanford.edu/projects/glove/)
  - [git](https://github.com/stanfordnlp/GloVe)
  

é¢„è®­ç»ƒ

- [ä¸é”™çš„åšä¸»](https://github.com/loujie0822/Pre-trained-Models)
  - [NLPç®—æ³•é¢è¯•å¿…å¤‡ï¼PTMsï¼šNLPé¢„è®­ç»ƒæ¨¡å‹çš„å…¨é¢æ€»ç»“](https://zhuanlan.zhihu.com/p/115014536)
  - [nlpä¸­çš„è¯å‘é‡å¯¹æ¯”ï¼šword2vec/glove/fastText/elmo/GPT/bert](https://zhuanlan.zhihu.com/p/56382372)
  - [nlpä¸­çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ€»ç»“(å•å‘æ¨¡å‹ã€BERTç³»åˆ—æ¨¡å‹ã€XLNet)](https://zhuanlan.zhihu.com/p/76912493)
- [Gloveè¯¦è§£](http://www.fanyeong.com/2018/02/19/glove-in-detail/#comment-1462)
- [ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹(NNLM)](https://blog.csdn.net/u010089444/article/details/52624964)

- [NLPå¿…è¯» | ååˆ†é’Ÿè¯»æ‡‚è°·æ­ŒBERTæ¨¡å‹](https://www.jianshu.com/p/4dbdb5ab959b)
- [Transformer-è®ºæ–‡è§£è¯»](https://www.jianshu.com/p/4b1bcd5c5f80)



# Transfomerç³»åˆ—

[å†å²ä»‹ç»](https://huggingface.co/course/en/chapter1/4?fw=pt#a-bit-of-transformer-history)


## åŸºç¡€çŸ¥è¯†

æ³¨æ„åŠ› 

[å‚è€ƒï¼š10.1. æ³¨æ„åŠ›æç¤º](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-cues.html)

å‚æ•°åŒ–å…¨è¿æ¥å±‚ã€éå‚æ•°åŒ–çš„poolingï¼Œéƒ½æ˜¯æå–è¯¥åŒºåŸŸæœ€å¤§æˆ–è€…å¹³å‡çš„ç‰¹å¾ï¼Œç›¸å½“äºè¢«è¿«çº³å…¥äº†æç¤ºï¼ˆåœ¨è‰ä¸›ä¸­çœ‹åˆ°çº¢è‰²èŠ±æœµï¼‰ã€‚
>é¦–å…ˆï¼Œè€ƒè™‘ä¸€ä¸ªç›¸å¯¹ç®€å•çš„çŠ¶å†µï¼Œ å³åªä½¿ç”¨éè‡ªä¸»æ€§æç¤ºã€‚ è¦æƒ³å°†é€‰æ‹©åå‘äºæ„Ÿå®˜è¾“å…¥ï¼Œ æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä½¿ç”¨å‚æ•°åŒ–çš„å…¨è¿æ¥å±‚ï¼Œ ç”šè‡³æ˜¯éå‚æ•°åŒ–çš„æœ€å¤§æ±‡èšå±‚æˆ–å¹³å‡æ±‡èšå±‚ã€‚â€œæ˜¯å¦åŒ…å«è‡ªä¸»æ€§æç¤ºâ€å°†æ³¨æ„åŠ›æœºåˆ¶ä¸å…¨è¿æ¥å±‚æˆ–æ±‡èšå±‚åŒºåˆ«å¼€æ¥ã€‚ åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å°†è‡ªä¸»æ€§æç¤ºç§°ä¸ºæŸ¥è¯¢ï¼ˆqueryï¼‰ã€‚ ç»™å®šä»»ä½•æŸ¥è¯¢ï¼Œæ³¨æ„åŠ›æœºåˆ¶é€šè¿‡æ³¨æ„åŠ›æ±‡èšï¼ˆattention poolingï¼‰ å°†é€‰æ‹©å¼•å¯¼è‡³æ„Ÿå®˜è¾“å…¥ï¼ˆsensory inputsï¼Œä¾‹å¦‚ä¸­é—´ç‰¹å¾è¡¨ç¤ºï¼‰ã€‚ åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œè¿™äº›æ„Ÿå®˜è¾“å…¥è¢«ç§°ä¸ºå€¼ï¼ˆvalueï¼‰ã€‚ æ›´é€šä¿—çš„è§£é‡Šï¼Œæ¯ä¸ªå€¼éƒ½ä¸ä¸€ä¸ªé”®ï¼ˆkeyï¼‰é…å¯¹ï¼Œ è¿™å¯ä»¥æƒ³è±¡ä¸ºæ„Ÿå®˜è¾“å…¥çš„éè‡ªä¸»æç¤ºã€‚ å¦‚ å›¾10.1.3æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡æ³¨æ„åŠ›æ±‡èšï¼Œ ä»¥ä¾¿ç»™å®šçš„æŸ¥è¯¢ï¼ˆè‡ªä¸»æ€§æç¤ºï¼‰å¯ä»¥ä¸é”®ï¼ˆéè‡ªä¸»æ€§æç¤ºï¼‰è¿›è¡ŒåŒ¹é…ï¼Œ è¿™å°†å¼•å¯¼å¾—å‡ºæœ€åŒ¹é…çš„å€¼ï¼ˆæ„Ÿå®˜è¾“å…¥ï¼‰ã€‚

æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°

[å‚è€ƒï¼š10.3. æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)

- æ©è”½softmaxæ“ä½œ
- åŠ æ€§æ³¨æ„åŠ›
- ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

å°ç»“ï¼š
- å°†æ³¨æ„åŠ›æ±‡èšçš„è¾“å‡ºè®¡ç®—å¯ä»¥ä½œä¸ºå€¼çš„åŠ æƒå¹³å‡ï¼Œé€‰æ‹©ä¸åŒçš„æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°ä¼šå¸¦æ¥ä¸åŒçš„æ³¨æ„åŠ›æ±‡èšæ“ä½œã€‚
- å½“æŸ¥è¯¢å’Œé”®æ˜¯ä¸åŒé•¿åº¦çš„çŸ¢é‡æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å¯åŠ æ€§æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°ã€‚å½“å®ƒä»¬çš„é•¿åº¦ç›¸åŒæ—¶ï¼Œä½¿ç”¨ç¼©æ”¾çš„â€œç‚¹ï¼ç§¯â€æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°çš„è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚

åœ¨è¿™ä¸ªæ•™ç¨‹é‡Œï¼Œä½“ç°äº†æŸ¥è¯¢qï¼Œåœ¨æ³¨æ„åŠ›é‡Œé¢å’Œ é”®-å€¼çš„è®¡ç®—å…³ç³»ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨ä»£ç é‡Œï¼Œæ‰©å±•åˆ°äº†é«˜ç»´åº¦çš„è®¡ç®—æ–¹æ³•ã€‚ï¼ˆä¸å¤ªå¥½æ‡‚ï¼Œéœ€è¦åå¤çœ‹ä»£ç  print(shape) ï¼‰

![attention-pic1.png](./pics/attention-pic1.png)

è¯¾åqaï¼š
- masked_softmaxçš„ä½œç”¨ï¼šæ¯”å¦‚ï¼Œåœ¨å¥å­ä¸å¤Ÿé•¿æ—¶ï¼ˆ4 out of 10ï¼‰ï¼Œpaddingå¡«å……äº†6ä¸ªã€‚é‚£ä¹ˆæ‹¿åˆ°queryæ—¶ï¼Œè¦çœ‹é”®å€¼ï¼Œæ­¤æ—¶ä»…éœ€è¦çœ‹å‰4ä¸ªï¼Œå6ä¸ªæ²¡æœ‰æ„ä¹‰ã€‚

å‚è€ƒï¼š
- [çŸ¥ä¹-transformerè§£æå’Œå®ç°](https://zhuanlan.zhihu.com/p/420820453)

## NERè¯†åˆ«

- [çŸ¥ä¹-BiLSTMä¸Šçš„CRFï¼Œç”¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡æ¥è§£é‡ŠCRFï¼ˆä¸€ï¼‰](https://zhuanlan.zhihu.com/p/119254570)
- [wx-ç¼ºå°‘è®­ç»ƒæ ·æœ¬æ€ä¹ˆåšå®ä½“è¯†åˆ«ï¼Ÿå°æ ·æœ¬ä¸‹çš„NERè§£å†³æ–¹æ³•æ±‡æ€»](https://mp.weixin.qq.com/s/FH1cWxXlTFt0RdEipSJH1w)

## é¢è¯•ç›¸å…³

- [NLPç®—æ³•-é¢ç»](https://cloud.tencent.com/developer/article/1817838)
- æ„Ÿè§‰æ•´ä½“æ•´ç†çš„ç›®å½•ä¸é”™ï¼šhttps://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/#tableofcontents
- NLPå²—ä½å…«è‚¡æ–‡ï¼šhttps://zhuanlan.zhihu.com/p/470674031

