{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有几个模块\n",
    "\n",
    "1. DataModule：实现数据模块，能够分批加载训练、验证数据\n",
    "2. SAGELightning：继承LightningModule的模块，用于sage的信息传递\n",
    "3. CrossEntropyLoss：定义损失函数，graphsage里的无监督训练\n",
    "4. UnsuptervisedClassfication：无监督训练，产出emb后进行下游的分类任务\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "from dgl.data import AsNodePredDataset\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora():\n",
    "    data0 = dgl.data.CSVDataset('../graph_dgl/cora_csv/')\n",
    "    data = AsNodePredDataset(data0, split_ratio=(0.5,0.2,0.3))\n",
    "    g = data[0]\n",
    "    g.ndata[\"features\"] = g.ndata.pop(\"feat\")\n",
    "    g.ndata[\"labels\"] = g.ndata.pop(\"label\")\n",
    "    return g, data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def load_reddit(self_loop=True, raw_dir='~/.dgl/'):\n",
    "    from dgl.data import RedditDataset\n",
    "\n",
    "    # load reddit data\n",
    "    data = RedditDataset(self_loop=self_loop, raw_dir=raw_dir)\n",
    "    g = data[0]\n",
    "    g.ndata[\"features\"] = g.ndata.pop(\"feat\")\n",
    "    g.ndata[\"labels\"] = g.ndata.pop(\"label\")\n",
    "    return g, data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "g, n_classes = load_cora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_edges = g.num_edges()\n",
    "reverse_eids = torch.cat([\n",
    "    torch.arange(n_edges // 2, n_edges),\n",
    "    torch.arange(0, n_edges // 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2714, 2715, 2716,  ..., 2711, 2712, 2713])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_eids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nid = th.nonzero(g.ndata['train_mask'], as_tuple=True)[0]\n",
    "val_nid = th.nonzero(g.ndata['val_mask'], as_tuple=True)[0]\n",
    "test_nid = th.nonzero(~(g.ndata['train_mask'] | g.ndata['val_mask']), as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inductive_split(g):\n",
    "    \"\"\"Split the graph into training graph, validation graph, and test graph by training\n",
    "    and validation masks.  Suitable for inductive models.\"\"\"\n",
    "    train_g = g.subgraph(g.ndata[\"train_mask\"])\n",
    "    val_g = g.subgraph(g.ndata[\"train_mask\"] | g.ndata[\"val_mask\"])\n",
    "    test_g = g\n",
    "    return train_g, val_g, test_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_out=[10, 25]\n",
    "base_sampler = dgl.dataloading.MultiLayerNeighborSampler([int(_) for _ in fan_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. nagetive sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler(object):\n",
    "    def __init__(self, g, k, neg_share=False, device=None):\n",
    "        if device is None:\n",
    "            device = g.device\n",
    "        self.weights = g.in_degrees().float().to(device) ** 0.75\n",
    "        self.k = k\n",
    "        self.neg_share = neg_share\n",
    "\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        n = len(src)\n",
    "        if self.neg_share and n % self.k == 0:\n",
    "            dst = self.weights.multinomial(n, replacement=True)\n",
    "            dst = dst.view(-1, 1, self.k).expand(-1, self.k, -1).flatten()\n",
    "        else:\n",
    "            dst = self.weights.multinomial(n * self.k, replacement=True)\n",
    "        src = src.repeat_interleave(self.k)\n",
    "        return src, dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            base_sampler, exclude='reverse_id',\n",
    "            reverse_eids=reverse_eids,\n",
    "            negative_sampler=NegativeSampler(g, 1, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloder = dgl.dataloading.DataLoader(\n",
    "            g,\n",
    "            np.arange(g.num_edges()),\n",
    "            sampler,\n",
    "            device=th.device('cpu'),\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(iter(train_dataloder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_batch) # input_nodes, pos_graph, neg_graph, mfgs = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 543, 1095,  552,  379,   28,   46,  435,  822,  331,   71,    0,  301,\n",
       "        1134,  589,  325,   97, 1543,  236,   15, 1713,  146,  424,    3, 1850,\n",
       "          62,  493,  577, 1039, 1304, 1287,  184, 1549, 1662, 1004,  486,  341,\n",
       "         857,  858, 1724,   73,  568, 1698,  618,  784, 1069,  470, 1040,  414,\n",
       "          74, 1210,  328, 1344,  713, 1077, 1305,  511, 1207,  579,  173,  672,\n",
       "         228,  259,   39,  479,  188, 1113,   47,   87, 1608,  642, 1712,  509,\n",
       "         513,  333, 1034,  921,   83,  812, 1502,  728, 1050,  527,  503, 1200,\n",
       "         529,  445,  619, 1710, 1491,  652,   76,  355,  195,   49,  452, 1184,\n",
       "         608,   13,  111,  121,  701,   52, 1716, 1111,  446,  898,  272, 1676,\n",
       "        1074,  279,  402, 1403,   40,  282,   64, 2065, 2685, 1758,  549, 2637,\n",
       "        2655,  845,  370,  737, 1754, 1126, 1485, 2251,  518,  324,   98, 2329,\n",
       "        1944,  790, 1952, 2461, 1101,  477, 2560,   37, 1942, 2001, 2143, 2266,\n",
       "        1925,   58, 1663,  546,  804,  560, 1612, 2378, 2502, 2451, 1515, 2390,\n",
       "        2182, 2436,  253,  521, 1769, 2297,  334, 1180, 2415,  162, 2426, 2288,\n",
       "        1596,  936, 1715, 2122, 1955, 1419,  980,   91,  523, 2099, 1441, 2067,\n",
       "         702,  120, 2371, 1586, 1714, 1446,  335, 2156, 1404, 2255,  916, 1746,\n",
       "        2551, 2466, 2606,  801,   36, 1438, 1389, 2624, 2596, 1585, 1347, 2445,\n",
       "        1616, 2304,  238, 2242,  398, 2246, 2404, 2108, 1109, 1185,  336, 1577,\n",
       "        2686, 2066, 1894,  719, 1505, 2401, 1930, 1576, 1673, 2433,  225, 2491,\n",
       "        1854, 1183, 2071, 1473, 2236, 1984, 2609, 2313, 1398,  952, 2641, 2074,\n",
       "        2627, 2470, 1964,  295,  874,  312,  217, 2053,  382, 1026, 1763,    6,\n",
       "        2305,  733, 2226, 2554, 2080, 1954,  861, 2656,  678, 1789, 1362, 2041,\n",
       "        1165, 1187, 1817, 1979,   59, 1063, 1506, 2091, 2083,   29, 2434,  913,\n",
       "        1888, 2227,  876, 2240, 1969, 2594,  245, 2342, 2150, 2620,   72,  584,\n",
       "         270, 2281, 1387,  473,  520, 1440, 2308, 2214,  854, 2478, 2425, 2231,\n",
       "        2486,  791,  911, 1261,  371,  100, 2139, 1068, 2441,  538, 1640,  567,\n",
       "        1881,  542, 2632,  556,  383, 2268,  920,  137,  375, 1418,  117, 1045,\n",
       "         389, 2337, 1172, 1682,  814, 2646,  896, 2507,  744,  614, 1681,  209,\n",
       "        2700, 1122, 2593, 1968, 1468, 2299,  625,  766,  448, 1064,  809, 1217,\n",
       "        1218,  136,  656,  827,   96,   99,  412,  729, 1540, 1541, 1542,  627,\n",
       "          14,  104,  106,  384,  786, 1706,    2,   85,  787,  788,  380,  838,\n",
       "        1589,  715,  716, 1548,  669,  340, 1562,  205,  464, 1067,  147,  617,\n",
       "         510,   68, 1190, 1546,   44,   78,  612, 1864, 1865, 1871, 1872, 1873,\n",
       "         928,  643,  712,  714,  122, 1169,  825,  200,   60,  517,  524,  657,\n",
       "         208,  196, 1035, 1036,  233,   84,  139,  240,  651,  975,  224,  757,\n",
       "         330,  703,  955,  268,  296,  351,  374,  792,  794,  211, 1182,  174,\n",
       "        1267, 1268, 1110, 1838,  269,  271,  756, 1313,  401,  404,   65,  286,\n",
       "         165,  378,  709,  118,  266,  332,   31,  583,  115,  251,  887,  976,\n",
       "        1274,  607,  889, 1447,  323, 1139,   26,  497, 1053,  260,  563,  564,\n",
       "         297,  298,  494,  320, 1021,  185, 1055,  805,  806,  863, 1605, 1780,\n",
       "        1401, 1513,  462,  559,  767, 1697,  516, 1191,    4,  171, 1328,  164,\n",
       "        1691, 1488, 1489, 1490,   27,  204,  411,  498,  505, 1052, 1275,   79,\n",
       "         397,  405,  192,  179,   16,   42,  186,  187,  189,  973,  436, 1494,\n",
       "         644, 1729, 1364, 1679,  409,  514,  648,  730,  935, 1215,   56, 1452,\n",
       "         407,  408,  417, 1156,   19,   43,   80,  419,  326, 1195, 1615, 1617,\n",
       "         484,  789,  793,  574, 1235,  114,  264,  884, 1252, 1579, 1580, 1042,\n",
       "        1479,  317,  925, 1493,  720,  234, 1654, 1672,  287,   67,  273,  304,\n",
       "         724, 1096,  132, 1516,  437, 1130,  168,  169, 1456,  482, 1129,  150,\n",
       "        1501, 1700,  381,  390,  675,   90,  154,  313,  796,  130, 1032,  366,\n",
       "         708, 1023, 1024, 1057, 1070, 1358,   88,  226, 1356,  108,  131,  489,\n",
       "          50, 1253, 1843,  322,  502,  166,  438,  869, 1071,  144,  504, 1289,\n",
       "         820,  821,  388,  284,  914,  915,   38,  203,  354,  696,  490, 1181,\n",
       "         358, 1047, 1145,  258,  421,   69,  466,  848, 1832,   30,  124,  877,\n",
       "        1246,  778,  653, 1028, 1044,  571, 1243, 1575,   35, 1174, 1314, 1317,\n",
       "         198,  535,  537, 1202,   45,  143,  459, 1120,  413,  440,  685, 1189,\n",
       "         426,  283, 1116,  152,  755, 1154, 1765, 1426, 1148, 1656,  485,  987,\n",
       "         988,  989,  990,  991, 1262, 1263, 1084,   63,  829, 1703,  465, 1112,\n",
       "         605,  663,   32, 1366,  749,  367, 1256, 1257, 1462,  919, 1435,  197,\n",
       "        1099, 1839, 1842,  102, 1665, 1161,  467, 1686,  897, 1578,  745,  613,\n",
       "         615,  453,  119,  368,  434,  508,  249,  250,  310, 1146,  581,  604,\n",
       "        1196, 1254,  573, 1329,  422,  655, 1121, 1321,   81,   54,  278,  105,\n",
       "         609, 1572,  450,  385,  878, 1559,  906,  807,  808,  706,  242,  316,\n",
       "         844,  984,  736, 1029,  285,   17,   18, 1761, 1467,  879,  265,  841,\n",
       "         247,  721,  110,  536, 1422,  471,  512,  161, 1281,  356,  357,  472,\n",
       "         582,   41,  637,  429,  746,    7, 1428,  649,  410,  418,  686, 1293,\n",
       "        1587, 1613,  126,  127,  977,  526,  300,  539,  798, 1555, 1740, 1741,\n",
       "          48,  222,  327, 1410, 1412,  365,  764,  833, 1037,  837,  707,  765,\n",
       "         221,  406,  140, 1815, 1417,   34, 1192, 1117,  294, 1013, 1014, 1136,\n",
       "         201,  772,  305,  763,  403,  548,  572,  175,  817, 1102,   66,   70,\n",
       "         620,  557, 1175,  176, 1738,  566,  309, 1010, 1296,  593,  337,  886,\n",
       "        1209,  214,  207, 1033, 1153])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
