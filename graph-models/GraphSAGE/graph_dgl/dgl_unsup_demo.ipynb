{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cde8ea-d413-4c5b-b7e5-d6732c06d7db",
   "metadata": {},
   "source": [
    "https://github.com/dmlc/dgl/blob/master/examples/pytorch/graphsage/advanced/train_lightning_unsupervised.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6aa707-c240-4c3b-bd43-0be1c38d2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import dgl.function as fn\n",
    "import time\n",
    "import argparse\n",
    "import tqdm\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7acbe0d-9ef5-4fbf-b29c-3a77f071cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as skm\n",
    "import torch as th\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_feats, n_hidden, n_classes, n_layers, activation, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init(in_feats, n_hidden, n_classes, n_layers, activation, dropout)\n",
    "\n",
    "    def init(\n",
    "        self, in_feats, n_hidden, n_classes, n_layers, activation, dropout\n",
    "    ):\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        if n_layers > 1:\n",
    "            self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, \"mean\"))\n",
    "            for i in range(1, n_layers - 1):\n",
    "                self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, \"mean\"))\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, \"mean\"))\n",
    "        else:\n",
    "            self.layers.append(dglnn.SAGEConv(in_feats, n_classes, \"mean\"))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h = layer(block, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, x, device, batch_size, num_workers):\n",
    "        \"\"\"\n",
    "        Inference with the GraphSAGE model on full neighbors (i.e. without neighbor sampling).\n",
    "        g : the entire graph.\n",
    "        x : the input of entire node set.\n",
    "\n",
    "        The inference code is written in a fashion that it could handle any number of nodes and\n",
    "        layers.\n",
    "        \"\"\"\n",
    "        # During inference with sampling, multi-layer blocks are very inefficient because\n",
    "        # lots of computations in the first few layers are repeated.\n",
    "        # Therefore, we compute the representation of all nodes layer by layer.  The nodes\n",
    "        # on each layer are of course splitted in batches.\n",
    "        # TODO: can we standardize this?\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = th.zeros(\n",
    "                g.num_nodes(),\n",
    "                self.n_hidden if l != len(self.layers) - 1 else self.n_classes,\n",
    "            )\n",
    "\n",
    "            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "            dataloader = dgl.dataloading.DataLoader(\n",
    "                g,\n",
    "                th.arange(g.num_nodes()).to(g.device),\n",
    "                sampler,\n",
    "                device=device if num_workers == 0 else None,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                num_workers=num_workers,\n",
    "            )\n",
    "\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                block = blocks[0]\n",
    "\n",
    "                block = block.int().to(device)\n",
    "                h = x[input_nodes].to(device)\n",
    "                h = layer(block, h)\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = self.activation(h)\n",
    "                    h = self.dropout(h)\n",
    "\n",
    "                y[output_nodes] = h.cpu()\n",
    "\n",
    "            x = y\n",
    "        return y\n",
    "\n",
    "\n",
    "def compute_acc_unsupervised(emb, labels, train_nids, val_nids, test_nids):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of prediction given the labels.\n",
    "    \"\"\"\n",
    "    emb = emb.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    train_nids = train_nids.cpu().numpy()\n",
    "    train_labels = labels[train_nids]\n",
    "    val_nids = val_nids.cpu().numpy()\n",
    "    val_labels = labels[val_nids]\n",
    "    test_nids = test_nids.cpu().numpy()\n",
    "    test_labels = labels[test_nids]\n",
    "\n",
    "    emb = (emb - emb.mean(0, keepdims=True)) / emb.std(0, keepdims=True)\n",
    "\n",
    "    lr = lm.LogisticRegression(multi_class=\"multinomial\", max_iter=10000)\n",
    "    lr.fit(emb[train_nids], train_labels)\n",
    "\n",
    "    pred = lr.predict(emb)\n",
    "    f1_micro_eval = skm.f1_score(val_labels, pred[val_nids], average=\"micro\")\n",
    "    f1_micro_test = skm.f1_score(test_labels, pred[test_nids], average=\"micro\")\n",
    "    return f1_micro_eval, f1_micro_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c2a530-f50d-45de-9899-7f59771730ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler(object):\n",
    "    def __init__(self, g, k, neg_share=False, device=None):\n",
    "        if device is None:\n",
    "            device = g.device\n",
    "        self.weights = g.in_degrees().float().to(device) ** 0.75\n",
    "        self.k = k\n",
    "        self.neg_share = neg_share\n",
    "\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        n = len(src)\n",
    "        if self.neg_share and n % self.k == 0:\n",
    "            dst = self.weights.multinomial(n, replacement=True)\n",
    "            dst = dst.view(-1, 1, self.k).expand(-1, self.k, -1).flatten()\n",
    "        else:\n",
    "            dst = self.weights.multinomial(n * self.k, replacement=True)\n",
    "        src = src.repeat_interleave(self.k)\n",
    "        return src, dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30cae467-7c3f-4163-bdc0-3295c7113234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "# from model import SAGE, compute_acc_unsupervised as compute_acc\n",
    "import sys\n",
    "# from load_graph import load_reddit, inductive_split, load_ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d668875f-be86-4b65-8b5c-60ecf7338de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "import dgl\n",
    "\n",
    "\n",
    "def load_reddit(self_loop=True):\n",
    "    from dgl.data import RedditDataset\n",
    "\n",
    "    # load reddit data\n",
    "    data = RedditDataset(self_loop=self_loop)\n",
    "    g = data[0]\n",
    "    g.ndata[\"features\"] = g.ndata.pop(\"feat\")\n",
    "    g.ndata[\"labels\"] = g.ndata.pop(\"label\")\n",
    "    return g, data.num_classes\n",
    "\n",
    "\n",
    "def load_ogb(name, root=\"dataset\"):\n",
    "    from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "    print(\"load\", name)\n",
    "    data = DglNodePropPredDataset(name=name, root=root)\n",
    "    print(\"finish loading\", name)\n",
    "    splitted_idx = data.get_idx_split()\n",
    "    graph, labels = data[0]\n",
    "    labels = labels[:, 0]\n",
    "\n",
    "    graph.ndata[\"features\"] = graph.ndata.pop(\"feat\")\n",
    "    graph.ndata[\"labels\"] = labels\n",
    "    in_feats = graph.ndata[\"features\"].shape[1]\n",
    "    num_labels = len(th.unique(labels[th.logical_not(th.isnan(labels))]))\n",
    "\n",
    "    # Find the node IDs in the training, validation, and test set.\n",
    "    train_nid, val_nid, test_nid = (\n",
    "        splitted_idx[\"train\"],\n",
    "        splitted_idx[\"valid\"],\n",
    "        splitted_idx[\"test\"],\n",
    "    )\n",
    "    train_mask = th.zeros((graph.number_of_nodes(),), dtype=th.bool)\n",
    "    train_mask[train_nid] = True\n",
    "    val_mask = th.zeros((graph.number_of_nodes(),), dtype=th.bool)\n",
    "    val_mask[val_nid] = True\n",
    "    test_mask = th.zeros((graph.number_of_nodes(),), dtype=th.bool)\n",
    "    test_mask[test_nid] = True\n",
    "    graph.ndata[\"train_mask\"] = train_mask\n",
    "    graph.ndata[\"val_mask\"] = val_mask\n",
    "    graph.ndata[\"test_mask\"] = test_mask\n",
    "    print(\"finish constructing\", name)\n",
    "    return graph, num_labels\n",
    "\n",
    "\n",
    "def inductive_split(g):\n",
    "    \"\"\"Split the graph into training graph, validation graph, and test graph by training\n",
    "    and validation masks.  Suitable for inductive models.\"\"\"\n",
    "    train_g = g.subgraph(g.ndata[\"train_mask\"])\n",
    "    val_g = g.subgraph(g.ndata[\"train_mask\"] | g.ndata[\"val_mask\"])\n",
    "    test_g = g\n",
    "    return train_g, val_g, test_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e70850-c103-4848-a6f2-02b5c72d4341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "245f1078-a0df-4fbf-a4c3-b660a8a97afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    def forward(self, block_outputs, pos_graph, neg_graph):\n",
    "        with pos_graph.local_scope():\n",
    "            pos_graph.ndata['h'] = block_outputs\n",
    "            pos_graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            pos_score = pos_graph.edata['score']\n",
    "        with neg_graph.local_scope():\n",
    "            neg_graph.ndata['h'] = block_outputs\n",
    "            neg_graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            neg_score = neg_graph.edata['score']\n",
    "\n",
    "        score = th.cat([pos_score, neg_score])\n",
    "        label = th.cat([th.ones_like(pos_score), th.zeros_like(neg_score)]).long()\n",
    "        loss = F.binary_cross_entropy_with_logits(score, label.float())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca0e9d-1dce-4bcb-9b40-d03a938144bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
